{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Train an Autoencoder for Anomaly Detection\n",
    "\n",
    "### Problem Statement\n",
    "You are tasked with implementing an **autoencoder** model for anomaly detection. The model will be trained on the **MNIST dataset**, and anomalies will be detected based on the reconstruction error. The autoencoder consists of an encoder to compress the input and a decoder to reconstruct the image. The difference between the original image and the reconstructed image will be used to detect anomalies.\n",
    "\n",
    "### Requirements\n",
    "1. **Define the Autoencoder Architecture**:\n",
    "   - **Encoder**:\n",
    "     - Implement a series of convolutional layers followed by max-pooling layers.\n",
    "     - The encoder should progressively reduce the spatial dimensions of the input image, capturing the most important features.\n",
    "   - **Decoder**:\n",
    "     - Implement a series of transposed convolutional layers (also known as deconvolutional layers) to upsample the compressed representation back to the original image size.\n",
    "     - Use a **Sigmoid activation** function in the final layer to ensure that the output pixel values are between 0 and 1.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Implement the forward method where the input image is passed through the encoder to obtain a compressed representation, followed by passing it through the decoder to reconstruct the image.\n",
    "\n",
    "### Constraints\n",
    "- The autoencoder should work on the MNIST dataset, which consists of 28x28 grayscale images.\n",
    "- Ensure that the output of the decoder matches the original image size.\n",
    "- Use **Sigmoid activation** in the final layer to constrain the output pixel values between 0 and 1.\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ’¡ Hint</summary>\n",
    "  Focus on the encoder to downsample the input and the decoder to upsample and reconstruct the image.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9912422/9912422 [00:29<00:00, 337163.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28881/28881 [00:00<00:00, 126134.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1648877/1648877 [00:05<00:00, 275673.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 2613241.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sequential in module torch.nn.modules.container:\n",
      "\n",
      "class Sequential(torch.nn.modules.module.Module)\n",
      " |  Sequential(*args)\n",
      " |  \n",
      " |  A sequential container.\n",
      " |  Modules will be added to it in the order they are passed in the\n",
      " |  constructor. Alternatively, an ``OrderedDict`` of modules can be\n",
      " |  passed in. The ``forward()`` method of ``Sequential`` accepts any\n",
      " |  input and forwards it to the first module it contains. It then\n",
      " |  \"chains\" outputs to inputs sequentially for each subsequent module,\n",
      " |  finally returning the output of the last module.\n",
      " |  \n",
      " |  The value a ``Sequential`` provides over manually calling a sequence\n",
      " |  of modules is that it allows treating the whole container as a\n",
      " |  single module, such that performing a transformation on the\n",
      " |  ``Sequential`` applies to each of the modules it stores (which are\n",
      " |  each a registered submodule of the ``Sequential``).\n",
      " |  \n",
      " |  What's the difference between a ``Sequential`` and a\n",
      " |  :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it\n",
      " |  sounds like--a list for storing ``Module`` s! On the other hand,\n",
      " |  the layers in a ``Sequential`` are connected in a cascading way.\n",
      " |  \n",
      " |  Example::\n",
      " |  \n",
      " |      # Using Sequential to create a small model. When `model` is run,\n",
      " |      # input will first be passed to `Conv2d(1,20,5)`. The output of\n",
      " |      # `Conv2d(1,20,5)` will be used as the input to the first\n",
      " |      # `ReLU`; the output of the first `ReLU` will become the input\n",
      " |      # for `Conv2d(20,64,5)`. Finally, the output of\n",
      " |      # `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n",
      " |      model = nn.Sequential(\n",
      " |                nn.Conv2d(1,20,5),\n",
      " |                nn.ReLU(),\n",
      " |                nn.Conv2d(20,64,5),\n",
      " |                nn.ReLU()\n",
      " |              )\n",
      " |  \n",
      " |      # Using Sequential with OrderedDict. This is functionally the\n",
      " |      # same as the above code\n",
      " |      model = nn.Sequential(OrderedDict([\n",
      " |                ('conv1', nn.Conv2d(1,20,5)),\n",
      " |                ('relu1', nn.ReLU()),\n",
      " |                ('conv2', nn.Conv2d(20,64,5)),\n",
      " |                ('relu2', nn.ReLU())\n",
      " |              ]))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other) -> 'Sequential'\n",
      " |  \n",
      " |  __delitem__(self, idx: Union[slice, int]) -> None\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getitem__(self, idx: Union[slice, int]) -> Union[ForwardRef('Sequential'), ~T]\n",
      " |  \n",
      " |  __iadd__(self, other) -> 'Sequential'\n",
      " |  \n",
      " |  __imul__(self, other: int) -> 'Sequential'\n",
      " |  \n",
      " |  __init__(self, *args)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  __iter__(self) -> Iterator[torch.nn.modules.module.Module]\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  __mul__(self, other: int) -> 'Sequential'\n",
      " |  \n",
      " |  __rmul__(self, other: int) -> 'Sequential'\n",
      " |  \n",
      " |  __setitem__(self, idx: int, module: torch.nn.modules.module.Module) -> None\n",
      " |  \n",
      " |  append(self, module: torch.nn.modules.module.Module) -> 'Sequential'\n",
      " |      Appends a given module to the end.\n",
      " |      \n",
      " |      Args:\n",
      " |          module (nn.Module): module to append\n",
      " |  \n",
      " |  extend(self, sequential) -> 'Sequential'\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  insert(self, index: int, module: torch.nn.modules.module.Module) -> 'Sequential'\n",
      " |  \n",
      " |  pop(self, key: Union[int, slice]) -> torch.nn.modules.module.Module\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_modules': typing.Dict[str, torch.nn.modules.modul...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_layer_parameters(layer):\n",
    "    return sum(p.numel() for p in layer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define an Autoencoder model\n",
    "# # TODO: Implement the autoencoder architecture\n",
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         # init\n",
    "#         super().__init__()\n",
    "#         ###########################################################################\n",
    "#         ############################## ENCODER LAYERS #############################\n",
    "#         ###########################################################################\n",
    "#         # Encoder - input image is of shape = batch_size x 1 x 28 x 28\n",
    "        \n",
    "#         conv_layer_11 = torch.nn.Conv2d(\n",
    "#             in_channels=1,\n",
    "#             out_channels=16,\n",
    "#             kernel_size=3,\n",
    "#             # stride=2,\n",
    "#             # padding\n",
    "#             # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "#         )   # output shape = batch_size x 16 x 26 x 26\n",
    "#         conv_layer_12 = torch.nn.Conv2d(\n",
    "#             in_channels=16,\n",
    "#             out_channels=16,\n",
    "#             kernel_size=3,\n",
    "#             # stride=2,\n",
    "#             # padding\n",
    "#             # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "#         )   # output shape = batch_size x 16 x 24 x 24\n",
    "#         max_pool_1 = torch.nn.MaxPool2d(\n",
    "#             kernel_size=2\n",
    "#         )   # output shape = batch_size x 16 x 12 x 12\n",
    "#         conv_layer_21 = torch.nn.Conv2d(\n",
    "#             in_channels=16,\n",
    "#             out_channels=32,\n",
    "#             kernel_size=5,\n",
    "#             # stride=2,\n",
    "#             # padding\n",
    "#             # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "#         )   # output shape = batch_size x 32 x 8 x 8\n",
    "#         conv_layer_22 = torch.nn.Conv2d(\n",
    "#             in_channels=32,\n",
    "#             out_channels=32,\n",
    "#             kernel_size=5,\n",
    "#             # stride=2,\n",
    "#             # padding\n",
    "#             # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "#         )   # output shape = batch_size x 32 x 4 x 4\n",
    "#         flatten = torch.nn.Flatten(\n",
    "#             start_dim = 1,\n",
    "#             end_dim = -1\n",
    "#         ) # output shape = batch_size x 512\n",
    "#         relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "#         ###########################################################################\n",
    "#         ############################## DECODER LAYERS #############################\n",
    "#         ###########################################################################\n",
    "#         unflatten = torch.nn.Unflatten(\n",
    "#             dim=1,\n",
    "#             unflattened_size=(32,4,4)\n",
    "#         ) # output shape = batch_size x 32 x 4 x 4\n",
    "#         deconv_layer_22 = torch.nn.ConvTranspose2d(\n",
    "#             in_channels=32,\n",
    "#             out_channels=32,\n",
    "#             kernel_size=5\n",
    "#         )   # output shape = batch_size x 32 x 8 x 8\n",
    "#         deconv_layer_21 = torch.nn.ConvTranspose2d(\n",
    "#             in_channels=32,\n",
    "#             out_channels=16,\n",
    "#             kernel_size=5\n",
    "#         )   # output shape = batch_size x 32 x 12 x 12\n",
    "#         # max_unpool_1 = torch.nn.MaxUnpool2d(\n",
    "#         #     kernel_size=2\n",
    "#         # )   # output shape = batch_size x 16 x 24 x 24\n",
    "#         upsample = torch.nn.Upsample(\n",
    "#             size=None,\n",
    "#             scale_factor=2,\n",
    "#             mode='nearest'\n",
    "#         )\n",
    "#         deconv_layer_12 = torch.nn.ConvTranspose2d(\n",
    "#             in_channels=16,\n",
    "#             out_channels=16,\n",
    "#             kernel_size=3\n",
    "#         )   # output shape = batch_size x 16 x 26 x 26\n",
    "#         deconv_layer_11 = torch.nn.ConvTranspose2d(\n",
    "#             in_channels=16,\n",
    "#             out_channels=1,\n",
    "#             kernel_size=3\n",
    "#         )   # output shape = batch_size x 1 x 28 x 28\n",
    "        \n",
    "        \n",
    "#         ###########################################################################\n",
    "#         ################################ AUTOENCODER ##############################\n",
    "#         ###########################################################################\n",
    "        \n",
    "#         # Encoder - Decoder layer\n",
    "#         self.encoder = torch.nn.Sequential(\n",
    "#             conv_layer_11,\n",
    "#             relu,\n",
    "#             conv_layer_12,\n",
    "#             relu,\n",
    "#             max_pool_1,\n",
    "#             conv_layer_21,\n",
    "#             relu,\n",
    "#             conv_layer_22,\n",
    "#             flatten\n",
    "#         )\n",
    "#         # Decoder\n",
    "#         self.decoder = torch.nn.Sequential(\n",
    "#             unflatten,\n",
    "#             deconv_layer_22,\n",
    "#             relu,\n",
    "#             deconv_layer_21,\n",
    "#             relu,\n",
    "#             upsample,\n",
    "#             deconv_layer_12,\n",
    "#             relu,\n",
    "#             deconv_layer_11\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define an Autoencoder model\n",
    "# # TODO: Implement the autoencoder architecture\n",
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         # init\n",
    "#         super().__init__()\n",
    "#         ###########################################################################\n",
    "#         ############################## ENCODER LAYERS #############################\n",
    "#         ###########################################################################\n",
    "#         # Encoder - input image is of shape = batch_size x 1 x 28 x 28\n",
    "        \n",
    "#         conv_layer_11 = torch.nn.Conv2d(\n",
    "#             in_channels=1,\n",
    "#             out_channels=8,\n",
    "#             kernel_size=3,\n",
    "#             stride=2,\n",
    "#             # padding\n",
    "#             # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "#         )   # output shape = batch_size x 16 x 13 x 13\n",
    "#         conv_layer_12 = torch.nn.Conv2d(\n",
    "#             in_channels=8,\n",
    "#             out_channels=8,\n",
    "#             kernel_size=4,\n",
    "#             stride=2,\n",
    "#             # padding\n",
    "#             # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "#         )   # output shape = batch_size x 8 x 5 x 5\n",
    "#         flatten = torch.nn.Flatten(\n",
    "#             start_dim = 1,\n",
    "#             end_dim = -1\n",
    "#         ) # output shape = batch_size x 200\n",
    "#         relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "#         ###########################################################################\n",
    "#         ############################## DECODER LAYERS #############################\n",
    "#         ###########################################################################\n",
    "#         unflatten = torch.nn.Unflatten(\n",
    "#             dim=1,\n",
    "#             unflattened_size=(8,5,5)\n",
    "#         ) # output shape = batch_size x 8 x 5 x 5\n",
    "#         deconv_layer_22 = torch.nn.ConvTranspose2d(\n",
    "#             in_channels=8,\n",
    "#             out_channels=8,\n",
    "#             kernel_size=5,\n",
    "#             stride=2\n",
    "#         )   # output shape = batch_size x 8 x 13 x 13\n",
    "#         deconv_layer_21 = torch.nn.ConvTranspose2d(\n",
    "#             in_channels=8,\n",
    "#             out_channels=1,\n",
    "#             kernel_size=4,\n",
    "#             stride=2\n",
    "#         )   # output shape = batch_size x 1 x 28 x 28\n",
    "        \n",
    "#         ###########################################################################\n",
    "#         ################################ AUTOENCODER ##############################\n",
    "#         ###########################################################################\n",
    "        \n",
    "#         # Encoder - Decoder layer\n",
    "#         self.encoder = torch.nn.Sequential(\n",
    "#             conv_layer_11,\n",
    "#             relu,\n",
    "#             conv_layer_12,\n",
    "#             flatten\n",
    "#         )\n",
    "#         # Decoder\n",
    "#         self.decoder = torch.nn.Sequential(\n",
    "#             unflatten,\n",
    "#             deconv_layer_22,\n",
    "#             relu,\n",
    "#             deconv_layer_21,\n",
    "            \n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm2d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm2d(_BatchNorm)\n",
      " |  BatchNorm2d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None) -> None\n",
      " |  \n",
      " |  Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\n",
      " |  with additional channel dimension) as described in the paper\n",
      " |  `Batch Normalization: Accelerating Deep Network Training by Reducing\n",
      " |  Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n",
      " |  of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\n",
      " |  to 1 and the elements of :math:`\\beta` are set to 0. The standard-deviation is calculated\n",
      " |  via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\n",
      " |  \n",
      " |  Also by default, during training this layer keeps running estimates of its\n",
      " |  computed mean and variance, which are then used for normalization during\n",
      " |  evaluation. The running estimates are kept with a default :attr:`momentum`\n",
      " |  of 0.1.\n",
      " |  \n",
      " |  If :attr:`track_running_stats` is set to ``False``, this layer then does not\n",
      " |  keep running estimates, and batch statistics are instead used during\n",
      " |  evaluation time as well.\n",
      " |  \n",
      " |  .. note::\n",
      " |      This :attr:`momentum` argument is different from one used in optimizer\n",
      " |      classes and the conventional notion of momentum. Mathematically, the\n",
      " |      update rule for running statistics here is\n",
      " |      :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n",
      " |      where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n",
      " |      new observed value.\n",
      " |  \n",
      " |  Because the Batch Normalization is done over the `C` dimension, computing statistics\n",
      " |  on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: :math:`C` from an expected input of size\n",
      " |          :math:`(N, C, H, W)`\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Can be set to ``None`` for cumulative moving average\n",
      " |          (i.e. simple average). Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, this module has\n",
      " |          learnable affine parameters. Default: ``True``\n",
      " |      track_running_stats: a boolean value that when set to ``True``, this\n",
      " |          module tracks the running mean and variance, and when set to ``False``,\n",
      " |          this module does not track such statistics, and initializes statistics\n",
      " |          buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n",
      " |          When these buffers are ``None``, this module always uses batch statistics.\n",
      " |          in both training and eval modes. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C, H, W)`\n",
      " |      - Output: :math:`(N, C, H, W)` (same shape as input)\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm2d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm2d(100, affine=False)\n",
      " |      >>> input = torch.randn(20, 100, 35, 45)\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm2d\n",
      " |      _BatchNorm\n",
      " |      _NormBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None) -> None\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor) -> torch.Tensor\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _NormBase:\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  reset_parameters(self) -> None\n",
      " |  \n",
      " |  reset_running_stats(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _NormBase:\n",
      " |  \n",
      " |  __constants__ = ['track_running_stats', 'momentum', 'eps', 'num_featur...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.BatchNorm2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an Autoencoder model\n",
    "# TODO: Implement the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        # init\n",
    "        super().__init__()\n",
    "        ###########################################################################\n",
    "        ############################## ENCODER LAYERS #############################\n",
    "        ###########################################################################\n",
    "        # Encoder - input image is of shape = batch_size x 1 x 28 x 28\n",
    "        \n",
    "        conv_layer_11 = torch.nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            # padding\n",
    "            # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "        )   # output shape = batch_size x 16 x 13 x 13\n",
    "        batch_norm_11 = torch.nn.BatchNorm2d(\n",
    "            num_features=8\n",
    "        )\n",
    "        conv_layer_12 = torch.nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=8,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            # padding\n",
    "            # in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None\n",
    "        )   # output shape = batch_size x 8 x 5 x 5\n",
    "        flatten = torch.nn.Flatten(\n",
    "            start_dim = 1,\n",
    "            end_dim = -1\n",
    "        ) # output shape = batch_size x 200\n",
    "        relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "        ###########################################################################\n",
    "        ############################## DECODER LAYERS #############################\n",
    "        ###########################################################################\n",
    "        unflatten = torch.nn.Unflatten(\n",
    "            dim=1,\n",
    "            unflattened_size=(8,5,5)\n",
    "        ) # output shape = batch_size x 8 x 5 x 5\n",
    "        deconv_layer_22 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=8,\n",
    "            out_channels=8,\n",
    "            kernel_size=5,\n",
    "            stride=2\n",
    "        )   # output shape = batch_size x 8 x 13 x 13\n",
    "        batch_norm_22 = torch.nn.BatchNorm2d(\n",
    "            num_features=8\n",
    "        )\n",
    "        deconv_layer_21 = torch.nn.ConvTranspose2d(\n",
    "            in_channels=8,\n",
    "            out_channels=1,\n",
    "            kernel_size=4,\n",
    "            stride=2\n",
    "        )   # output shape = batch_size x 1 x 28 x 28\n",
    "        ###########################################################################\n",
    "        ################################ AUTOENCODER ##############################\n",
    "        ###########################################################################\n",
    "        \n",
    "        # Encoder - Decoder layer\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            conv_layer_11,\n",
    "            batch_norm_11,\n",
    "            relu,\n",
    "            conv_layer_12,\n",
    "            flatten\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            unflatten,\n",
    "            deconv_layer_22,\n",
    "            batch_norm_22,\n",
    "            relu,\n",
    "            deconv_layer_21,\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 8, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Unflatten(dim=1, unflattened_size=(8, 5, 5))\n",
      "    (1): ConvTranspose2d(8, 8, kernel_size=(5, 5), stride=(2, 2))\n",
      "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Autoencoder                              [64, 1, 28, 28]           --\n",
       "â”œâ”€Sequential: 1-1                        [64, 200]                 1,032\n",
       "â”‚    â””â”€Conv2d: 2-1                       [64, 8, 13, 13]           80\n",
       "â”‚    â””â”€BatchNorm2d: 2-2                  [64, 8, 13, 13]           16\n",
       "â”œâ”€Sequential: 1-2                        --                        (recursive)\n",
       "â”‚    â””â”€ReLU: 2-3                         [64, 8, 13, 13]           --\n",
       "â”œâ”€Sequential: 1-3                        --                        (recursive)\n",
       "â”‚    â””â”€Conv2d: 2-4                       [64, 8, 5, 5]             1,032\n",
       "â”‚    â””â”€Flatten: 2-5                      [64, 200]                 --\n",
       "â”œâ”€Sequential: 1-4                        [64, 1, 28, 28]           --\n",
       "â”‚    â””â”€Unflatten: 2-6                    [64, 8, 5, 5]             --\n",
       "â”‚    â””â”€ConvTranspose2d: 2-7              [64, 8, 13, 13]           1,608\n",
       "â”‚    â””â”€BatchNorm2d: 2-8                  [64, 8, 13, 13]           16\n",
       "â”‚    â””â”€ReLU: 2-9                         [64, 8, 13, 13]           --\n",
       "â”‚    â””â”€ConvTranspose2d: 2-10             [64, 1, 28, 28]           129\n",
       "==========================================================================================\n",
       "Total params: 3,913\n",
       "Trainable params: 3,913\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 26.38\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 3.27\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 3.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(64,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0280\n",
      "Epoch [2/10], Loss: 0.0243\n",
      "Epoch [3/10], Loss: 0.0236\n",
      "Epoch [4/10], Loss: 0.0208\n",
      "Epoch [5/10], Loss: 0.0183\n",
      "Epoch [6/10], Loss: 0.0204\n",
      "Epoch [7/10], Loss: 0.0177\n",
      "Epoch [8/10], Loss: 0.0162\n",
      "Epoch [9/10], Loss: 0.0159\n",
      "Epoch [10/10], Loss: 0.0164\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, _ in train_loader:\n",
    "        # Forward pass\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9249\n",
      "Epoch [2/10], Loss: 0.9258\n",
      "Epoch [3/10], Loss: 0.9226\n",
      "Epoch [4/10], Loss: 0.9253\n",
      "Epoch [5/10], Loss: 0.9297\n",
      "Epoch [6/10], Loss: 0.9214\n",
      "Epoch [7/10], Loss: 0.9269\n",
      "Epoch [8/10], Loss: 0.9277\n",
      "Epoch [9/10], Loss: 0.9277\n",
      "Epoch [10/10], Loss: 0.9269\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, _ in train_loader:\n",
    "        # Forward pass\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0271\n",
      "Epoch [2/10], Loss: 0.0178\n",
      "Epoch [3/10], Loss: 0.0180\n",
      "Epoch [4/10], Loss: 0.0165\n",
      "Epoch [5/10], Loss: 0.0156\n",
      "Epoch [6/10], Loss: 0.0146\n",
      "Epoch [7/10], Loss: 0.0123\n",
      "Epoch [8/10], Loss: 0.0126\n",
      "Epoch [9/10], Loss: 0.0127\n",
      "Epoch [10/10], Loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, _ in train_loader:\n",
    "        # Forward pass\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise ground truth and reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using reconstruction error\n",
    "threshold = 0.001  # Define a threshold for anomaly detection\n",
    "model.eval()\n",
    "gt_reconstructed_images = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        reconstructed = model(images)\n",
    "        loss = nn.MSELoss(reduction='none')(reconstructed, images).mean(dim=(1,2,3))\n",
    "        gt_reconstructed_images.extend(zip(images, reconstructed, loss))\n",
    "        # gt_reconstructed_images.append([images, reconstructed, loss])\n",
    "gt_reconstructed_images = sorted(gt_reconstructed_images, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGFCAYAAABdSJFpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHYElEQVR4nO3deXQUVdoG8Kc6S2dfyUpCCJFNIDBGQAz7FoIyIFEGcEkQhmEMKKCguLHIMoIDjIgw6gyoAy4gi4PIvokKKpsCAQkGZAuQQBKSkK37fn/wpSdNd26l00lle37n1Dnpurdu3aruvnm7uu7bihBCgIiIiEgjupruABERETUsDD6IiIhIUww+iIiISFMMPoiIiEhTDD6IiIhIUww+iIiISFMMPoiIiEhTDD6IiIhIUww+iIiISFMMPogaoKSkJDRt2lTz/Z47dw6KomDlypWa75uIag8GH0TV4PLly5gxYwaOHj3aoPvQUGVlZWHs2LEICAiAu7s7evXqhcOHD1d4+5SUFAwYMAAeHh7w8/PDk08+ievXr0u3WbVqFRRFgYeHh0VZUlISFEWxWFq1amVWb8aMGVbrlS7ffvtthY+BSMaxpjtAVB9dvnwZM2fORNOmTdGhQ4da14f3338fRqOxRvpV3xmNRjz00EM4duwYpkyZgkaNGuHdd99Fz549cejQITRv3ly6/cWLF9G9e3d4e3tj7ty5yM3NxVtvvYVffvkFP/zwA5ydnS22yc3NxdSpU+Hu7l5uu3q9Hh988IHZOm9vb7PHQ4cOxT333GOx7csvv4zc3Fx07NhR2neiimLwQbVKXl6edACtr/Lz8+Hm5qbZ/pycnDTbV0Ozdu1afPfdd1izZg0effRRAMCwYcPQokULTJ8+HatXr5ZuP3fuXOTl5eHQoUNo0qQJAKBTp07o168fVq5cibFjx1psM3v2bHh6eqJXr17YsGGD1XYdHR3xxBNPSPcdHR2N6Ohos3UXLlzAxYsXMWbMGKuBD1Fl8GsXqjGll3hPnjyJkSNHwtfXF127djWV/+c//0FMTAxcXV3h5+eH4cOH48KFCxbtHDx4EAMHDoSvry/c3d0RHR2Nf/zjH2Z1du3ahW7dusHd3R0+Pj4YPHgwUlJSrPYnNTUVSUlJ8PHxgbe3N0aNGoX8/Hyzutu3b0fXrl3h4+MDDw8PtGzZEi+//DIAYM+ePaZPiKNGjTJdsi69z6Fnz55o27YtDh06hO7du8PNzc20raIomDFjhsUxNm3aFElJSWbrsrKyMGnSJDRt2hR6vR5hYWF46qmnkJGRodoHa/d85OXl4fnnn0d4eDj0ej1atmyJt956C3f/8LWiKBg/fjw2bNiAtm3bQq/Xo02bNtiyZYtFvyuqIs/PrVu3MHHiRNPxBgYGol+/fmZfZ5w5cwYJCQkIDg6Gi4sLwsLCMHz4cGRnZ1e6b7Zau3YtgoKCMHToUNO6gIAADBs2DBs3bkRhYaF0+y+++AIPP/ywKfAAgL59+6JFixb4/PPPLeqfOXMGixYtwsKFC+HoKP88aTAYkJOTY9PxfPLJJxBC4PHHH7dpOyIZXvmgGvfYY4+hefPmmDt3rukf3Zw5c/Daa69h2LBhGDNmDK5fv44lS5age/fuOHLkCHx8fADcCQIefvhhhISE4LnnnkNwcDBSUlKwadMmPPfccwCAHTt2ID4+Hs2aNcOMGTNw+/ZtLFmyBLGxsTh8+LDFP+Fhw4YhMjIS8+bNw+HDh/HBBx8gMDAQb775JgDgxIkTePjhhxEdHY1Zs2ZBr9cjNTXV9H1469atMWvWLLz++usYO3YsunXrBgB48MEHTfvIzMxEfHw8hg8fjieeeAJBQUE2nbPc3Fx069YNKSkpePrpp3HfffchIyMDX375JS5evFihPpQlhMAf//hH7N69G6NHj0aHDh2wdetWTJkyBZcuXcKiRYvM6u/fvx/r1q3DM888A09PT7z99ttISEjA77//Dn9/f5uOpaLPz7hx47B27VqMHz8e9957LzIzM7F//36kpKTgvvvuQ1FREeLi4lBYWIgJEyYgODgYly5dwqZNm5CVlWXxFUNZ+fn5FgGmNQ4ODvD19ZXWOXLkCO677z7odOaf7Tp16oT33nsPv/76K9q1a2d120uXLuHatWu4//77Lco6deqEzZs3W6yfOHEievXqhYEDB1oNTkrl5+fDy8sL+fn58PX1xYgRI/Dmm29avUekrFWrViE8PBzdu3eX1iOyiSCqIdOnTxcAxIgRI8zWnzt3Tjg4OIg5c+aYrf/ll1+Eo6OjaX1JSYmIjIwUERER4ubNm2Z1jUaj6e8OHTqIwMBAkZmZaVp37NgxodPpxFNPPWXRn6efftqsrUceeUT4+/ubHi9atEgAENevXy/32H788UcBQKxYscKirEePHgKAWL58uUUZADF9+nSL9RERESIxMdH0+PXXXxcAxLp16yzqlh67rA+JiYkiIiLC9HjDhg0CgJg9e7ZZvUcffVQoiiJSU1PN+ujs7Gy27tixYwKAWLJkicW+ykpLS7PoU0WfH29vb5GcnFxu20eOHBEAxJo1a6R9sKb0uVdbyp6z8ri7u1u8hoQQ4quvvhIAxJYtW8rdtvQ5++ijjyzKpkyZIgCIgoIC07pNmzYJR0dHceLECSHEnefV3d3dYtuXXnpJvPjii+Kzzz4Tn3zyiUhMTBQARGxsrCguLi63P8ePHxcAxNSpU6XHTGQrXvmgGjdu3Dizx+vWrYPRaMSwYcOQkZFhWh8cHIzmzZtj9+7dePnll3HkyBGkpaVh0aJFpishpRRFAQBcuXIFR48exdSpU+Hn52cqj46ORr9+/ax+kry7P926dcP69euRk5MDLy8v0742btyIUaNGWXzCrQi9Xo9Ro0bZvF2pL774Au3bt8cjjzxiUVZ67LbYvHkzHBwc8Oyzz5qtf/7557F27Vp8/fXXGD9+vGl93759ERUVZXocHR0NLy8v/Pbbbzbt15bnx8fHBwcPHsTly5cRGhpq0VbplY2tW7di4MCBNt1D89RTT5l95VceV1dX1Tq3b9+GXq+3WO/i4mIql20LQHV7vV6PoqIiTJo0CePGjcO9994r7dO8efPMHg8fPhwtWrTAK6+8grVr12L48OFWt1u1ahUA8CsXqnK854NqXGRkpNnjM2fOQAiB5s2bIyAgwGxJSUnBtWvXAABnz54FALRt27bcts+fPw8AaNmypUVZ69atkZGRgby8PLP1Zb9rB2C6zH7z5k0AwJ/+9CfExsZizJgxCAoKwvDhw/H555/bNHukcePGdt28d/bsWelx2+r8+fMIDQ2Fp6en2frWrVubysu6+xwBd85T6TmyZb9AxZ6f+fPn4/jx4wgPD0enTp0wY8YMs2AnMjISkydPxgcffIBGjRohLi4OS5curdD9Hs2aNUPfvn1Vl9jYWNW2XF1drd7XUVBQYCqXbQugQtsvWrQIGRkZmDlzpmqfrJk0aRJ0Oh127NhhtVwIgdWrV6Nt27YWN6ES2YtXPqjG3T0YG41GKIqCr7/+Gg4ODhb11b6jtpe1fQIw3Y/i6uqKffv2Yffu3fjqq6+wZcsWfPbZZ+jduze2bdtW7vZlVeQTdFkGg8Gm+tVN7RxVh2HDhpmuQm3btg0LFizAm2++iXXr1iE+Ph4A8Pe//x1JSUnYuHEjtm3bhmeffRbz5s3DgQMHEBYWVm7bubm5yM3NVe2Dg4MDAgICpHVCQkJw5coVi/Wl66xdtSm7bdm6d2/v5+cHvV6P7OxszJ49G8888wxycnJMN5Hm5uZCCIFz587Bzc0NgYGB5e7L1dUV/v7+uHHjhtXyb7/9FufPn7e4akJUFXjlg2qdqKgoCCEQGRlp9dPnAw88YKoHAMePHy+3rYiICADA6dOnLcpOnTqFRo0aVWpqr06nQ58+fbBw4UKcPHkSc+bMwa5du7B7924AlfvqA7hz9SArK8tsXVFRkcU/o6ioKOlx29qHiIgIXL58Gbdu3TJbf+rUKVN5dbD1+QkJCcEzzzyDDRs2IC0tDf7+/pgzZ47Zdu3atcOrr76Kffv24ZtvvsGlS5ewfPlyaT/eeusthISEqC4VyXPRoUMHHD582OJK2MGDB+Hm5oYWLVqUu23jxo0REBCAn376yaLshx9+MOVruXnzJnJzczF//nxERkaali+++AL5+fmIjIy0OiW3rFu3biEjI6PcYKo0adnIkSNVjpjIdgw+qNYZOnQoHBwcMHPmTItP0kIIZGZmAgDuu+8+REZGYvHixRb/sEu3CwkJQYcOHfDhhx+a1Tl+/Di2bduGgQMH2tw/a58US/8plF4uL/2HeXe/1ERFRWHfvn1m69577z2LKx8JCQk4duwY1q9fb9FG6bHb0oeBAwfCYDDgnXfeMVu/aNEiKIpiurJQ1Sr6/BgMBouvTwIDAxEaGmo65zk5OSgpKTGr065dO+h0OtXprU899RS2b9+uupTeAyHz6KOP4urVq1i3bp1pXUZGBtasWYNBgwaZ3c9x9uxZ09eHpRISErBp0yazaeU7d+7Er7/+iscee8x07OvXr7dYevXqBRcXF6xfvx7Tpk0DcOfrmruDSgB44403IITAgAEDLMqKi4uxZs0adO3a1epXbET24tcuVOtERUVh9uzZmDZtGs6dO4chQ4bA09MTaWlpWL9+PcaOHYsXXngBOp0Oy5Ytw6BBg9ChQweMGjUKISEhOHXqFE6cOIGtW7cCABYsWID4+Hh06dIFo0ePNk3l9Pb2tppTQ82sWbOwb98+PPTQQ4iIiMC1a9fw7rvvIiwszHTTYlRUFHx8fLB8+XJ4enrC3d0dnTt3tri/5W5jxozBuHHjkJCQgH79+uHYsWPYunUrGjVqZFZvypQpWLt2LR577DE8/fTTiImJwY0bN/Dll19i+fLlaN++vU19GDRoEHr16oVXXnkF586dQ/v27bFt2zZs3LgREydONLu5tKpV5Pm5desWwsLC8Oijj6J9+/bw8PDAjh078OOPP+Lvf/87gDu5QsaPH4/HHnsMLVq0QElJCT7++GM4ODggISFB2odmzZqhWbNmVXI8jz76KB544AGMGjUKJ0+eNGU4NRgMFvdn9OnTB8Cd37wp9fLLL2PNmjXo1asXnnvuOeTm5mLBggVo166d6SZlNzc3DBkyxGLfGzZswA8//GBWlp6ejj/84Q8YMWKEKZ361q1bsXnzZgwYMACDBw+2aGfr1q3IzMzkjaZUfWpolg2RaXpjeVNWv/jiC9G1a1fh7u4u3N3dRatWrURycrI4ffq0Wb39+/eLfv36CU9PT+Hu7i6io6Mtpnzu2LFDxMbGCldXV+Hl5SUGDRokTp48WaH+rFixQgAQaWlpQgghdu7cKQYPHixCQ0OFs7OzCA0NFSNGjBC//vqr2XYbN24U9957r3B0dDSbXtqjRw/Rpk0bq8dsMBjEiy++KBo1aiTc3NxEXFycSE1NtZhqK4QQmZmZYvz48aJx48bC2dlZhIWFicTERJGRkaHah7un2gohxK1bt8SkSZNEaGiocHJyEs2bNxcLFiwwm7YsxJ2pttamvFrr492sTbUVQv35KSwsFFOmTBHt27c3Pc/t27cX7777rqnOb7/9Jp5++mkRFRUlXFxchJ+fn+jVq5fYsWOHtE/V4caNG2L06NHC399fuLm5iR49eogff/zRol5ERITV6bvHjx8X/fv3F25ubsLHx0c8/vjjIj09XXW/1qba3rx5UzzxxBPinnvuEW5ubkKv14s2bdqIuXPniqKiIqvtDB8+XDg5OZlNfyaqSooQ1XiHGBEREdFdeM8HERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQdVu5UrV0JRFJw7d66mu2LVnj17oCgK1q5dW9NdISINzJgxA4qiqNZLSkpC06ZNq79DDRCDD6o1Nm/ejBkzZlRb+6tXr8bixYurrX2i+uLy5cuYMWMGjh492qD7QNWHwQfVGps3b8bMmTOrrX0GH0QVc/nyZcycObPGg4+a7sP777+P06dP19j+6zMGH/WE0WhEQUFBTXdDMyUlJSgqKqrpbhCZycvLq+ku1Ij8/Pya7kK1cHJygl6vr+lu1EsMPmqZPXv24P7774eLiwuioqLwz3/+0+r3k4qiYPz48Vi1ahXatGkDvV6PLVu2AACOHDmC+Ph4eHl5wcPDA3369MGBAwfMti/vO09r92c0bdoUDz/8MPbv349OnTrBxcUFzZo1w0cffWSx/YkTJ9C7d2+4uroiLCwMs2fPhtFoVD3upKQkLF261HRspQsAnDt3Doqi4K233sLixYsRFRUFvV6PkydPlns/Sel9HHv27AEA9OzZE1999RXOnz9vavvu73KNRiPmzJmDsLAwuLi4oE+fPkhNTVXtOzVMpe+hkydPYuTIkfD19UXXrl1N5f/5z38QExMDV1dX+Pn5Yfjw4bhw4YJFOwcPHsTAgQPh6+sLd3d3REdH4x//+IdZnV27dqFbt25wd3eHj48PBg8ejJSUFKv9SU1NRVJSEnx8fODt7Y1Ro0ZZBAfbt29H165d4ePjAw8PD7Rs2RIvv/wygDvvnY4dOwIARo0aZXq/rFy5EsCd91Lbtm1x6NAhdO/eHW5ubqZtFUWx+tVp06ZNkZSUZLYuKysLkyZNQtOmTaHX6xEWFoannnoKGRkZqn0oPW8DBgyAt7c33Nzc0KNHD3z77bcW+96/fz86duxoNqZW1N33fJQdi5YuXYpmzZrBzc0N/fv3x4ULFyCEwBtvvIGwsDC4urpi8ODBuHHjhlmbGzduxEMPPYTQ0FDo9XpERUXhjTfegMFgsNh/6T5cXV3RqVMnfPPNN+jZsyd69uxpVq+wsBDTp0/HPffcA71ej/DwcEydOhWFhYUVPlatOdZ0B+h/jhw5ggEDBiAkJAQzZ86EwWDArFmzEBAQYLX+rl278Pnnn2P8+PFo1KgRmjZtihMnTqBbt27w8vLC1KlT4eTkhH/+85/o2bMn9u7di86dO1eqb6mpqXj00UcxevRoJCYm4t///jeSkpIQExODNm3aAADS09PRq1cvlJSU4KWXXoK7uzvee+89uLq6qrb/l7/8BZcvX8b27dvx8ccfW62zYsUKFBQUYOzYsdDr9fDz86tw/1955RVkZ2fj4sWLWLRoEQDAw8PDrM7f/vY36HQ6vPDCC8jOzsb8+fPx+OOP4+DBgxXeDzU8jz32GJo3b465c+dCCAEAmDNnDl577TUMGzYMY8aMwfXr17FkyRJ0794dR44cgY+PD4A7QcDDDz+MkJAQPPfccwgODkZKSgo2bdqE5557DgCwY8cOxMfHo1mzZpgxYwZu376NJUuWIDY2FocPH7YIoocNG4bIyEjMmzcPhw8fxgcffIDAwEC8+eabAO58QHj44YcRHR2NWbNmQa/XIzU11fSPu3Xr1pg1axZef/11jB07Ft26dQMAPPjgg6Z9ZGZmIj4+HsOHD8cTTzyBoKAgm85Zbm4uunXrhpSUFDz99NO47777kJGRgS+//BIXL15U7cOuXbsQHx+PmJgYTJ8+HTqdDitWrEDv3r3xzTffoFOnTgCAX375Bf3790dAQABmzJiBkpISTJ8+3eb+3m3VqlUoKirChAkTcOPGDcyfPx/Dhg1D7969sWfPHrz44otITU3FkiVL8MILL+Df//63aduVK1fCw8MDkydPhoeHB3bt2oXXX38dOTk5WLBgganesmXLMH78eHTr1g2TJk3CuXPnMGTIEPj6+iIsLMxUz2g04o9//CP279+PsWPHonXr1vjll1+waNEi/Prrr9iwYYNdx1ptBNUagwYNEm5ubuLSpUumdWfOnBGOjo7i7qcKgNDpdOLEiRNm64cMGSKcnZ3F2bNnTesuX74sPD09Rffu3U3rpk+fbtGmEEKsWLFCABBpaWmmdREREQKA2Ldvn2ndtWvXhF6vF88//7xp3cSJEwUAcfDgQbN63t7eFm1ak5ycbLVPaWlpAoDw8vIS165dU+2vEELs3r1bABC7d+82rXvooYdERESERfuldVu3bi0KCwtN6//xj38IAOKXX36R9psaptL30IgRI8zWnzt3Tjg4OIg5c+aYrf/ll1+Eo6OjaX1JSYmIjIwUERER4ubNm2Z1jUaj6e8OHTqIwMBAkZmZaVp37NgxodPpxFNPPWXRn6efftqsrUceeUT4+/ubHi9atEgAENevXy/32H788UcBQKxYscKirEePHgKAWL58uUUZADF9+nSL9RERESIxMdH0+PXXXxcAxLp16yzqlh57eX0wGo2iefPmIi4uzuw85efni8jISNGvXz/TuiFDhggXFxdx/vx507qTJ08KBwcHq2PN3RITE83GjNKxKCAgQGRlZZnWT5s2TQAQ7du3F8XFxab1I0aMEM7OzqKgoMCsn3f7y1/+Itzc3Ez1CgsLhb+/v+jYsaNZeytXrhQARI8ePUzrPv74Y6HT6cQ333xj1uby5csFAPHtt9+qHmdN4NcutYTBYMCOHTswZMgQhIaGmtbfc889iI+Pt7pNjx49cO+995q1sW3bNgwZMgTNmjUzrQ8JCcHIkSOxf/9+5OTkVKp/9957r+nTBwAEBASgZcuW+O2330zrNm/ejAceeMD0qaO03uOPP16pfd4tISGh3KtAVWHUqFFwdnY2PS493rLHSHS3cePGmT1et24djEYjhg0bhoyMDNMSHByM5s2bY/fu3QDuXOlMS0vDxIkTTVdCSpV+5XjlyhUcPXoUSUlJZlf6oqOj0a9fP2zevFm1P926dUNmZqbpvV+6r40bN1boK1Fr9Ho9Ro0aValtAeCLL75A+/bt8cgjj1iUqU2BPXr0KM6cOYORI0ciMzPTdH7z8vLQp08f7Nu3D0ajEQaDAVu3bsWQIUPQpEkT0/atW7dGXFxcpfsO3Lna5e3tbXpcekX5iSeegKOjo9n6oqIiXLp0ybSu7JXgW7duISMjA926dUN+fj5OnToFAPjpp5+QmZmJP//5z2btPf744/D19TXry5o1a9C6dWu0atXK7PXWu3dvADC93mobfu1SS1y7dg23b9/GPffcY1FmbR0AREZGmj2+fv068vPz0bJlS4u6rVu3htFoxIULF0xfk9ii7Ju3lK+vL27evGl6fP78eatf61jrT2XcfbxV7e5jLH2Tlz1Gorvd/bo8c+YMhBBo3ry51fpOTk4AgLNnzwIA2rZtW27b58+fB2D9PdS6dWts3boVeXl5cHd3N62XvY69vLzwpz/9CR988AHGjBmDl156CX369MHQoUPx6KOPQqer2OfRxo0bmwXqtjp79iwSEhIqte2ZM2cAAImJieXWyc7ORmFhIW7fvm31eWjZsqXVwK2i7j7HpYFIeHi41fVlx5ATJ07g1Vdfxa5duyw+DGZnZwP43/N+99jv6Oho8TXbmTNnkJKSUu4Hs2vXrlXkkDTH4KMOq8i9FOUp79OFtZueAMDBwcHqevH/33Frwdrx2nocMrXhGKnuuft1aTQaoSgKvv76a6uvqbvvNapqaq9jV1dX7Nu3D7t378ZXX32FLVu24LPPPkPv3r2xbdu2crcvy9axpzLvx/KUXq1ZsGABOnToYLWOh4dHtd5sWd45Ujv3WVlZ6NGjB7y8vDBr1ixERUXBxcUFhw8fxosvvlipK1FGoxHt2rXDwoULrZbfHRDVFgw+aonAwEC4uLhYnV1R0RkXAQEBcHNzszov/dSpU9DpdKYXYumnoaysLLNLvqURd2VERESYPpWUVdF58hXJOHi3ssdRlrXjqEz7RLaKioqCEAKRkZFo0aKFtB4AHD9+HH379rVaJyIiAoD199CpU6fQqFEjs6seFaXT6dCnTx/06dMHCxcuxNy5c/HKK69g9+7d6Nu3b6XfK76+vhbvxaKiIly5csVsXVRUFI4fPy5tq7w+lJ43Ly+vcs8bcGc8dHV1tWtMqmp79uxBZmYm1q1bh+7du5vWp6WlmdUrfd5TU1PRq1cv0/qSkhKcO3cO0dHRpnVRUVE4duwY+vTpU6fGON7zUUs4ODigb9++2LBhAy5fvmxan5qaiq+//rrCbfTv3x8bN240m3p69epVrF69Gl27doWXlxeA/72B9+3bZ6qXl5eHDz/8sNLHMHDgQBw4cAA//PCDad3169exatWqCm1fOojePXjJWDsOg8GA9957z2r7pZc1iarL0KFD4eDggJkzZ1pcNRNCIDMzEwBw3333ITIyEosXL7Z4zZduFxISgg4dOuDDDz80q3P8+HFs27YNAwcOtLl/d0/9BGC6glB6taAy70Xgzvux7HsRAN577z2LKx8JCQk4duwY1q9fb9FG6bGX14eYmBhERUXhrbfeQm5ursX2169fB3BnPIyLi8OGDRvw+++/m8pTUlKwdetWm46rqpReGSn7uigqKsK7775rVu/++++Hv78/3n//fZSUlJjWr1q1yuJr4GHDhuHSpUt4//33LfZ3+/btWpt7hlc+apEZM2Zg27ZtiI2NxV//+lcYDAa88847aNu2bYWz/M2ePds0h/+ZZ56Bo6Mj/vnPf6KwsBDz58831evfvz+aNGmC0aNHY8qUKXBwcMC///1vBAQEmL1RbTF16lR8/PHHGDBgAJ577jnTVNuIiAj8/PPPqtvHxMQAAJ599lnExcXBwcEBw4cPl27Tpk0bPPDAA5g2bRpu3LgBPz8/fPrpp2Zv2LLtf/bZZ5g8eTI6duwIDw8PDBo0qFLHSlSeqKgozJ49G9OmTTNNj/T09ERaWhrWr1+PsWPH4oUXXoBOp8OyZcswaNAgdOjQAaNGjUJISAhOnTqFEydOmP5BLliwAPHx8ejSpQtGjx5tmmrr7e1dqZ8jmDVrFvbt24eHHnoIERERuHbtGt59912EhYWZ8pRERUXBx8cHy5cvh6enJ9zd3dG5c2fV+67GjBmDcePGISEhAf369cOxY8ewdetWNGrUyKzelClTsHbtWjz22GN4+umnERMTgxs3buDLL7/E8uXL0b59e2kfPvjgA8THx6NNmzYYNWoUGjdujEuXLmH37t3w8vLCf//7XwDAzJkzsWXLFnTr1g3PPPMMSkpKsGTJErRp06ZCY1JVe/DBB+Hr64vExEQ8++yzUBQFH3/8sUWQ6uzsjBkzZmDChAno3bs3hg0bhnPnzmHlypWIiooyu8Lx5JNP4vPPP8e4ceOwe/duxMbGwmAw4NSpU/j888+xdetW3H///VofqrqamWRD5dm5c6f4wx/+IJydnUVUVJT44IMPxPPPPy9cXFzM6gEQycnJVts4fPiwiIuLEx4eHsLNzU306tVLfPfddxb1Dh06JDp37iycnZ1FkyZNxMKFC8udavvQQw9ZbN+jRw+zKV9CCPHzzz+LHj16CBcXF9G4cWPxxhtviH/9618VmmpbUlIiJkyYIAICAoSiKKapcKXT2xYsWGB1u7Nnz4q+ffsKvV4vgoKCxMsvvyy2b99uMdU2NzdXjBw5Uvj4+AgApil0pVNt16xZY9Zu6X6tTTckKp3aWt6U1S+++EJ07dpVuLu7C3d3d9GqVSuRnJwsTp8+bVZv//79ol+/fsLT01O4u7uL6OhosWTJErM6O3bsELGxscLV1VV4eXmJQYMGiZMnT1aoP3e/p3fu3CkGDx4sQkNDhbOzswgNDRUjRowQv/76q9l2GzduFPfee69pqn/p+6BHjx6iTZs2Vo/ZYDCIF198UTRq1Ei4ubmJuLg4kZqaajHVVgghMjMzxfjx40Xjxo2Fs7OzCAsLE4mJiSIjI0O1D0IIceTIETF06FDh7+8v9Hq9iIiIEMOGDRM7d+4028/evXtFTEyMcHZ2Fs2aNRPLly8vN9XA3cqbanv3WFTeGFJ67n/88UfTum+//VY88MADwtXVVYSGhoqpU6eKrVu3WoxXQgjx9ttvi4iICKHX60WnTp3Et99+K2JiYsSAAQPM6hUVFYk333xTtGnTRuj1euHr6ytiYmLEzJkzRXZ2tupx1gRFCN5NV9sNGTIEJ06csPrdJRERNQxGoxEBAQEYOnSo1a9Z6hLe81HL3L592+zxmTNnsHnzZot0ukREVH8VFBRYfB3z0Ucf4caNG/Xi/wGvfNQyISEhSEpKQrNmzXD+/HksW7YMhYWFOHLkSLl5A4iIqH7Zs2cPJk2ahMceewz+/v44fPgw/vWvf6F169Y4dOiQXXlWagPecFrLDBgwAJ988gnS09Oh1+vRpUsXzJ07l4EHEVED0rRpU4SHh+Ptt9823Uz/1FNP4W9/+1udDzwAXvkgIiIijfGeDyIiItIUgw8iIiLSVK2758NoNOLy5cvw9PSsU6liieoTIQRu3bqF0NDQCv/YWE3j2EFUs2waN6orgcg777xjlhzl4MGDFdruwoULAgAXLlxqwXLhwoXqGiKsquy4IQTHDi5castSkXGjWq58lKawXr58OTp37ozFixcjLi4Op0+fRmBgoHRbT09PAEBXDIQjnKqje0SkogTF2I/NpvejFuwZN4D/jR09AxLhqKuh2QD14YqL2hyE6j7Ghr7/quiDzs4+GlX2X077JcYi7ElfUaFxo1pmu3Tu3BkdO3bEO++8A+DO5dDw8HBMmDABL730knTbnJwceHt7oycGw1Fh8EFUE0pEMfZgI7Kzs00/Rljd7Bk3gP+NHX2D/szgwx41/c+3oe+/Kvpg71elRmOl2i8xFmLH5X9WaNyo8i9zi4qKcOjQIbOfOtbpdOjbty++//57i/qFhYXIyckxW4ioYbF13AA4dhDVZVUefGRkZMBgMCAoKMhsfVBQENLT0y3qz5s3D97e3qYlPDy8qrtERLWcreMGwLGDqC6r8dvYp02bhuzsbNNy4cKFmu4SEdUBHDuI6q4qv+G0UaNGcHBwwNWrV83WX716FcHBwRb19Xo99Hp9VXeDiOoQW8cNgGMHUV1W5Vc+nJ2dERMTg507d5rWGY1G7Ny5E126dKnq3RFRPcBx4//pdPJFC4oiX2p6/0LIl+revxbs7YPRKF/UaPA6rJaptpMnT0ZiYiLuv/9+dOrUCYsXL0ZeXh5GjRpVHbsjonqA4wZRw1Etwcef/vQnXL9+Ha+//jrS09PRoUMHbNmyxeJmMiKiUhw3iBqOWvertszzQVTzaiLPh73qRZ4PtUvaFblkXt/VhjwcdV01fYVXo3k+iIiIiGQYfBAREZGmGHwQERGRphh8EBERkaaqZbYLEVGDZO/9+waD/X1Qu+Gyds0xsJnaHInqvt1Udf9VccOrShtC5cZjpaZuXFb7NdwyeOWDiIiINMXgg4iIiDTF4IOIiIg0xeCDiIiINMXgg4iIiDTF4IOIiIg0xeCDiIiINMU8H0RE/0+THA7VvX97f3iture3U40/B2o5NFTKRUlJRTpRrX1QzSfj4CAvr4I8IbzyQURERJpi8EFERESaYvBBREREmmLwQURERJpi8EFERESaYvBBREREmmLwQURERJping8iqleEwQghrOchUBzszH/gaOeQqZY/QS0HRFXk+VDd3M5cI9Wch0M1h4VaDgq17SuSh0PG2UlarJqjA4AoKpJXsPMcqD7Hau+D8huucFVe+SAiIiJNMfggIiIiTTH4ICIiIk0x+CAiIiJNMfggIiIiTTH4ICIiIk0x+CAiIiJNMc8HEdUvOuXOYoUwqOSAUOSfx4RKDgjF3U3efqE8f4PiJM8RIYqL5e0DUNRykajkgFDrg2oeDLVcJmp5ONS2V8tBYef2irOztFwtB4dSUCjfv17ePgAori7yPtwukDeg9jpVOUZ7c8VURJVf+ZgxYwYURTFbWrVqVdW7IaJ6hOMGUcNSLVc+2rRpgx07dvxvJ/ZmBSSieo/jBlHDUS3vbkdHRwQHB1dH00RUT3HcIGo4quWG0zNnziA0NBTNmjXD448/jt9//73cuoWFhcjJyTFbiKjhsWXcADh2ENVlVR58dO7cGStXrsSWLVuwbNkypKWloVu3brh165bV+vPmzYO3t7dpCQ8Pr+ouEVEtZ+u4AXDsIKrLFKH283Z2ysrKQkREBBYuXIjRo0dblBcWFqKw8H93B+fk5CA8PBw9MRiOispd10RULUpEMfZgI7Kzs+Hl5aX5/tXGDaD8saNP4Bg46sq5m99o53BXzq/llrJ3tovar+ZqMdvF7l/ereOzXVSfA7XZLmq/OFuB2S5qautslxJjEXZcfb9C40a139Hl4+ODFi1aIDU11Wq5Xq+HXq+v7m4QUR2iNm4AHDuI6rJqDz5yc3Nx9uxZPPnkk9W9KyKqJ6pt3Cgn/0dFKTqVq7FF6lcmpNQ+sap9qgbUryyofapVuzKgqJxDte3VqF1ZUdu/o/zKh1DLtSJvXfXKklB5DSiOKs8PAKhc4VK9uqX2OlF7Daid4yr4wqTK7/l44YUXsHfvXpw7dw7fffcdHnnkETg4OGDEiBFVvSsiqic4bhA1LFV+5ePixYsYMWIEMjMzERAQgK5du+LAgQMICAio6l0RUT3BcYOoYany4OPTTz+t6iaJqJ7juEHUsPCH5YiIiEhTDD6IiIhIUww+iIiISFMMPoiIiEhT/NlIIqpXFEWBUk6eAnsTOgu1HBoqKpSnQ9oB9f6rHaNqH9T2YWcGU7VzqJrDQiWPSMnvl6Tljk1V0vCrHb9aubeHvPxGtrwcUM2CKgoKpeV2v87UlJcHRC0/SBm88kFERESaYvBBREREmmLwQURERJpi8EFERESaYvBBREREmmLwQURERJpi8EFERESaYvBBREREmmKSsWqgOMkTxCguevU2HORxoSgqljdgZzIk2JmkRnF1sWv7839pJS2/3VieaOirhxZJywd9mywtj3riqLS8IsmeqIbonQCd9fegYlR53tTeN2oJrq5lSMsdQ4Kk5aKkRFquuLtJywFALc2TUBlbkHFT3r6Pl7z9W7ny7b08peUlgfL2HTLl7V9/pJm0/Ga0/Dls3eqitDzlXIh8+ynnpOVCLUkb1J9Du9mQDKy68MoHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaapO5vlw8PeTlive8nnivyeESsvz2hTa3KeyIsOuS8ufDDug2kZnl3PS8vU5f5CWpxfJz4FOkec7CHS6JS3PNchzlbwRuF9arm6HndvL84yc7vkvafkfI4ZIy0vO/W5rh0gj4lYehGI9D46il+fggZOTvO1C+diQP/h+afnl7vL8Cq4R8vddbNhv0nIACHCW58HIKpbnCjl+U57HoqXPVWn5+Vz5+NzC65K0PNzlF2n5fa7npOUOkI9tWUb58Td3ko/f6/3lY++uDrHScqcdh6TlAKDzlOdC0an8j1PLR1Mb8MoHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERacrmPB/79u3DggULcOjQIVy5cgXr16/HkCFDTOVCCEyfPh3vv/8+srKyEBsbi2XLlqF58+ZV1ulee89Lyyf7nqmyfdUceZ6KF/1T7GrdQZHHnQZhtKt9e10x5EvLE38dKS2f1WyDtPwBeZoSpLwgz3XQfDzzfNhCy3FDcXaGorOez0P4+0i3LW4kzwHhmK2S5yNQ/r7q2Om0tHxO2JfS8jPF/tJyACgQ8lwlWQb5Mfb1OSEtdyknh0qpIl8HaXlKQWNpeZyHfP/XDe7S8uXpPaXlP1+R53l6vo08x1C0q/y9/+n9vaXlESny/QOAKCmRV1ArV+T5ZGoDm6985OXloX379li6dKnV8vnz5+Ptt9/G8uXLcfDgQbi7uyMuLg4FBQV2d5aI6iaOG0RUls1XPuLj4xEfH2+1TAiBxYsX49VXX8XgwYMBAB999BGCgoKwYcMGDB8+3L7eElGdxHGDiMqq0ns+0tLSkJ6ejr59+5rWeXt7o3Pnzvj++++tblNYWIicnByzhYgajsqMGwDHDqK6rEqDj/T0dABAUFCQ2fqgoCBT2d3mzZsHb29v0xIeHl6VXSKiWq4y4wbAsYOoLqvx2S7Tpk1Ddna2ablw4UJNd4mI6gCOHUR1V5UGH8HBwQCAq1fNf/Xw6tWrprK76fV6eHl5mS1E1HBUZtwAOHYQ1WVVGnxERkYiODgYO3fuNK3LycnBwYMH0aVLl6rcFRHVExw3iBoem2e75ObmIjU11fQ4LS0NR48ehZ+fH5o0aYKJEydi9uzZaN68OSIjI/Haa68hNDTUbE6/vZYf6S4tv+/Bc9Jyf508h8SvxYHS8pTb8nnqalq7XlKt08LpmrQ80yifq78/t6W0PNcgT3Sx7WP5oH+rqTwPiN9x++aZu18xSMtdNv0gLT97Sv4cPqCXn9/AA7V/nnxdoum4oSjl5jlQbsvzdAhFnkOiIFhenmvnbSfzr/aTll8v8FBtI8ojQ1reWH9TWv7v87HS8mKj/DNr1oEgabmikqLiP4Xyc+BxST72eKfmSctD3eV5UD6cJh/73mrxubS8yEtIy+Eoz4MCAGqjjyiS51pRKrAPKV1lr0tUfNy0Ofj46aef0KtXL9PjyZMnAwASExOxcuVKTJ06FXl5eRg7diyysrLQtWtXbNmyBS4u8qRZRFR/cdwgorJsDj569uwJIcqP7BRFwaxZszBr1iy7OkZE9QfHDSIqq8ZnuxAREVHDwuCDiIiINMXgg4iIiDTF4IOIiIg0xeCDiIiINGXzbJfa4J4nj0jL32o/TFpu8JDnuHC6LJ8HX5J2Xlqu5ofInqp1ikN9peUOufJ8BcZjKSp7kM9FD8Z3KuX127UH5OfH+z8adYRsJkoMEDrreWKU2wXSbfVnyv8tGQAQbvKpvxG35FlWcz4JkJcXy5NgCGd5jgoA+MVJnuPmRJF8Hx4qfcCNLGmxn2OutFzk35a376oyvVrlOYST/N+as8rxebi4SsvPFMlHP8d8lVwXhUXy8gpQHKr5uoFRnkul/O1UcpyUwSsfREREpCkGH0RERKQpBh9ERESkKQYfREREpCkGH0RERKQpBh9ERESkKQYfREREpKk6medDjVqOC5VZ2FCZ5W63iuQJUVTqVHIWNlG9pzg6QNE5WC0TJdbzf5joVN5ZOfIcFk4q7YubWfL2VShKBT4vlqiMYI7yYV9xkedBEgaVc6SSRwkqeT4UZ2eV/as8hw7Wn3tT+67yPB6++hxpeVOn69JyofIUieJieQUAios814lQeY4VRe2/XM3jlQ8iIiLSFIMPIiIi0hSDDyIiItIUgw8iIiLSFIMPIiIi0hSDDyIiItIUgw8iIiLSVL3M80FEDZgQdxYrFAeVz1tq+RFUcmSgoFDevEqOiaqgQCXPhr3tu6kcQ7FKDgpPD/n2ajks9Cp5SPLypOWGrGxp+dXb/tLyAuEkLS/xsP7aK6W4uUnLAQAquUDqQh4PNbzyQURERJpi8EFERESaYvBBREREmmLwQURERJpi8EFERESaYvBBREREmmLwQURERJqyOfjYt28fBg0ahNDQUCiKgg0bNpiVJyUlQVEUs2XAgAFV1V8iqoM0HTcUpfxFhTAapQtKSuRLaY6R8hY1Op180YJaH4xG+VLD+1dcXaWLQyN/6WIw6qRLgXCSLgZXo3SBk6P64ihfhBDSpdqfgypgcyt5eXlo3749li5dWm6dAQMG4MqVK6blk08+sauTRFS3cdwgorJsznAaHx+P+Ph4aR29Xo/g4OBKd4qI6heOG0RUVrVcx9uzZw8CAwPRsmVL/PWvf0VmZmZ17IaI6hGOG0QNR5X/tsuAAQMwdOhQREZG4uzZs3j55ZcRHx+P77//Hg4ODhb1CwsLUVj4v99DyMnJqeouEVEtZ+u4AXDsIKrLqjz4GD58uOnvdu3aITo6GlFRUdizZw/69OljUX/evHmYOXNmVXeDiOoQW8cNgGMHUV1W7bdPN2vWDI0aNUJqaqrV8mnTpiE7O9u0XLhwobq7RES1nNq4AXDsIKrLqvzKx90uXryIzMxMhISEWC3X6/XQq/xEMhE1LGrjBsCxg6guszn4yM3NNfs0kpaWhqNHj8LPzw9+fn6YOXMmEhISEBwcjLNnz2Lq1Km45557EBcXV6UdJ6K6Q9Nxo6I5NawxyrcTsC+PhWqmEYPBrvYB2J3nQbE3T4S9x1CBfCwy4vZteQWV59jdSR7QGoT8CwPHWypfKNwukJcDqq9fxc5zpErtOayC/dscfPz000/o1auX6fHkyZMBAImJiVi2bBl+/vlnfPjhh8jKykJoaCj69++PN954g59QiBowjhtEVJbNwUfPnj2lkfXWrVvt6hAR1T8cN4ioLP62CxEREWmKwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpqtqTjBER1Rk6+/IXqOVfUMvBYe/2VdWGXdRyQNi7f7XtVaZnK47yf3tujvnScn+HXHn7KikyREGhvAIAxd1N3kZxsXx71R2o1KjuPCLglQ8iIiLSGIMPIiIi0hSDDyIiItIUgw8iIiLSFIMPIiIi0hSDDyIiItIUgw8iIiLSFPN8UL3koMhzATgojLvrKyFEubks1HJgqObIMBjl5Xa+rKoiB4dqG0Z5udoxqJ0jtTwcqrlO5K2rKymRFhsyb0jL80t87du9h8rxebhXoBH5MSi6uj9+1f0jICIiojqFwQcRERFpisEHERERaYrBBxEREWmKwQcRERFpisEHERERaYrBBxEREWmKeT6oXjIIebYAg5Dna6C6S9Hpys2DIIz2Pe+Kg/zzmr15OlTzjFRBHpDq7oNqHhD1DsjL1c6BwSAtdggLlZb76zOl5ZdU8oDoilXOX36+tBwAFGdn1TryTqhcV1A5R3BwkJfb+T4CeOWDiIiINMbgg4iIiDTF4IOIiIg0xeCDiIiINMXgg4iIiDTF4IOIiIg0xeCDiIiINMU8H1QnFfeNkZbHuu5XacGt6jpDtYsQ5eaCKC//h9m2drA7x4UW7TvY14a9fVB9DtQ7IC0WBnkOirx2wdJyP+ffpeXOijxHhmOevH8VyeEhCgrlFRzl/7oVlf/saudI7X1Q7mvAhvePTa+CefPmoWPHjvD09ERgYCCGDBmC06dPm9UpKChAcnIy/P394eHhgYSEBFy9etWW3RBRPcOxg4jKsin42Lt3L5KTk3HgwAFs374dxcXF6N+/P/Ly8kx1Jk2ahP/+979Ys2YN9u7di8uXL2Po0KFV3nEiqjs4dhBRWTZ97bJlyxazxytXrkRgYCAOHTqE7t27Izs7G//617+wevVq9O7dGwCwYsUKtG7dGgcOHMADDzxQdT0nojqDYwcRlWXXl2/Z2dkAAD8/PwDAoUOHUFxcjL59+5rqtGrVCk2aNMH3339vtY3CwkLk5OSYLURUv3HsIGrYKh18GI1GTJw4EbGxsWjbti0AID09Hc7OzvDx8TGrGxQUhPT0dKvtzJs3D97e3qYlPDy8sl0iojqAYwcRVTr4SE5OxvHjx/Hpp5/a1YFp06YhOzvbtFy4cMGu9oioduPYQUSVmmo7fvx4bNq0Cfv27UNYWJhpfXBwMIqKipCVlWX2Cebq1asIDrY+vUmv10Ov11emG0RUx3DsICLAxuBDCIEJEyZg/fr12LNnDyIjI83KY2Ji4OTkhJ07dyIhIQEAcPr0afz+++/o0qVL1fWaGrzsKPlc+aaOzONRm2g6dihK+bkg7MzjUaF9y6jt397ta0Mf7N3ewUFebpTnqND5+UjLC3zk7TdyuiUtb+l0TVouVL5PMGar35uk8/eT7yM3T15eUiItV/TquUak7ZfzHJa33hqbgo/k5GSsXr0aGzduhKenp+m7WG9vb7i6usLb2xujR4/G5MmT4efnBy8vL0yYMAFdunTh3epEDRjHDiIqy6bgY9myZQCAnj17mq1fsWIFkpKSAACLFi2CTqdDQkICCgsLERcXh3fffbdKOktEdRPHDiIqy+avXdS4uLhg6dKlWLp0aaU7RUT1C8cOIiqLPyxHREREmmLwQURERJpi8EFERESaYvBBREREmmLwQURERJqqVIZTovrO9ZJKoiOqvYSotmRiarN2VNJrVWQH8nK1BF4VaUOLPtizvcFg1/bGG1nycqcwaXmEc4a0vLWzPIGhSo4yKO7u8goAoJYkzEWe2VeoJGKzl1LOc1Deemt45YOIiIg0xeCDiIiINMXgg4iIiDTF4IOIiIg0xeCDiIiINMXgg4iIiDTF4IOIiIg0xTwfRFZELDshLVfJREA1SVHKzwVhZw4MW/IYVHIH8vJqyl9SpX2o7jwgKvvXBfhLy3OayZv30eVLy4en9ZaWh225IS0XBQXyDgBQvDzlFVTyeKi9TlXz1VRy+4r8enUpXvkgIiIiTTH4ICIiIk0x+CAiIiJNMfggIiIiTTH4ICIiIk0x+CAiIiJNMfggIiIiTTHPBzVIH98KlpaL4hKNekKaUstfoJI/Qb35Gs4DAlR/LpDqPka1/js5yTe/cVNa7pkWIi2f+9tD0vKLx+Tbtyi8Ki1X3N2k5QAAQ/VmElJ09l13KO91rthwPYNXPoiIiEhTDD6IiIhIUww+iIiISFMMPoiIiEhTDD6IiIhIUww+iIiISFMMPoiIiEhTNuX5mDdvHtatW4dTp07B1dUVDz74IN588020bNnSVKdnz57Yu3ev2XZ/+ctfsHz58qrpMVEV2JzRTlp+aVxLaXnI37+ryu7Ue5qOHUKUnyvCzhwVank8hEqOCtX8Cmo5Lqoix4ZaG1r0wQ6ioFBarvj5Ssu9LhRLy69vD5WW+9xQeY7zC6TlwqCeS0ZxdJBXsPc5UtveQWX/dubDAWy88rF3714kJyfjwIED2L59O4qLi9G/f3/k5eWZ1fvzn/+MK1eumJb58+fb3VEiqrs4dhBRWTZd+diyZYvZ45UrVyIwMBCHDh1C9+7dTevd3NwQHCzPIElEDQfHDiIqy657PrKzswEAfn5+ZutXrVqFRo0aoW3btpg2bRry8/PLbaOwsBA5OTlmCxHVbxw7iBq2Sv+2i9FoxMSJExEbG4u2bdua1o8cORIREREIDQ3Fzz//jBdffBGnT5/GunXrrLYzb948zJw5s7LdIKI6hmMHEVU6+EhOTsbx48exf/9+s/Vjx441/d2uXTuEhISgT58+OHv2LKKioizamTZtGiZPnmx6nJOTg/Dw8Mp2i4hqOY4dRFSp4GP8+PHYtGkT9u3bh7CwMGndzp07AwBSU1OtDiB6vR56vb4y3SCiOoZjBxEBNgYfQghMmDAB69evx549exAZGam6zdGjRwEAISHynyEmovqLYwcRlWVT8JGcnIzVq1dj48aN8PT0RHp6OgDA29sbrq6uOHv2LFavXo2BAwfC398fP//8MyZNmoTu3bsjOjq6Wg6AGibf0/K5/r8Wy+fafxK5XVp+Txv5P0f+O7SNpmOHolQ+F4VRnv9A2JuWUS1/QkmJvFwtTwgAGAwV709NUDsGledOqfTNAnfor5V/EzMAhG3Lk5YrRSrPkVr/XSpwtU5nZy6VInkuE7tztZT7HFa8XZuexmXLlgG4kwyorBUrViApKQnOzs7YsWMHFi9ejLy8PISHhyMhIQGvvvqqLbshonqGYwcRlWXz1y4y4eHhFhkKiYg4dhBRWfxtFyIiItIUgw8iIiLSFIMPIiIi0hSDDyIiItIUgw8iIiLSlJ0zpolqhsOew9LyHwuaSMtX3pRn12w9+Yy0vJZnUmjYdEr5eQiMRummiqM8D4dQ297VRVqOAnl+GjiqDMlqeUCqoo3q3l4tD4laLhQ1t+U5fnTF8v6LoiJpueLuJt+/g8pneoP8NQQAUKtibx4Qe5X3PlDJk1MWr3wQERGRphh8EBERkaYYfBAREZGmGHwQERGRphh8EBERkaYYfBAREZGmat1U29IfoCpBMVDxWTtEZm7nyqfTFd6W/+R0iZBPtzMIlZ+sruNKcOf41H4QrjYxjR1GyXNnw1RAq/tQm2prVJkCKesbABhVpqGqlVdFG9VerjKPVLHzM7Fa+yr9E0b5e1sxqkwFtvf4gQr85L3a66wCU7Ltat/6+6j0vVeRcUMRtWx0uXjxIsLDw2u6G0QE4MKFCwgLk+dEqS04dhDVDhUZN2pd8GE0GnH58mV4enpCURTk5OQgPDwcFy5cgJeXV013r07iObRPQzx/QgjcunULoaGh0JWXsKuW4dhRtXj+7NfQzqEt40at+9pFp9NZjZi8vLwaxJNXnXgO7dPQzp+3t3dNd8EmHDuqB8+f/RrSOazouFE3PtIQERFRvcHgg4iIiDRV64MPvV6P6dOnQ6/X13RX6iyeQ/vw/NVNfN7sw/NnP57D8tW6G06JiIiofqv1Vz6IiIiofmHwQURERJpi8EFERESaYvBBREREmqr1wcfSpUvRtGlTuLi4oHPnzvjhhx9quku11r59+zBo0CCEhoZCURRs2LDBrFwIgddffx0hISFwdXVF3759cebMmZrpbC00b948dOzYEZ6enggMDMSQIUNw+vRpszoFBQVITk6Gv78/PDw8kJCQgKtXr9ZQj6k8HDcqjuOGfThuVE6tDj4+++wzTJ48GdOnT8fhw4fRvn17xMXF4dq1azXdtVopLy8P7du3x9KlS62Wz58/H2+//TaWL1+OgwcPwt3dHXFxcSgoKNC4p7XT3r17kZycjAMHDmD79u0oLi5G//79kZeXZ6ozadIk/Pe//8WaNWuwd+9eXL58GUOHDq3BXtPdOG7YhuOGfThuVJKoxTp16iSSk5NNjw0GgwgNDRXz5s2rwV7VDQDE+vXrTY+NRqMIDg4WCxYsMK3LysoSer1efPLJJzXQw9rv2rVrAoDYu3evEOLO+XJychJr1qwx1UlJSREAxPfff19T3aS7cNyoPI4b9uO4UTG19spHUVERDh06hL59+5rW6XQ69O3bF99//30N9qxuSktLQ3p6utn59Pb2RufOnXk+y5GdnQ0A8PPzAwAcOnQIxcXFZuewVatWaNKkCc9hLcFxo2px3LAdx42KqbXBR0ZGBgwGA4KCgszWBwUFIT09vYZ6VXeVnjOez4oxGo2YOHEiYmNj0bZtWwB3zqGzszN8fHzM6vIc1h4cN6oWxw3bcNyouFr3q7ZEtUFycjKOHz+O/fv313RXiKiO4LhRcbX2ykejRo3g4OBgcUfw1atXERwcXEO9qrtKzxnPp7rx48dj06ZN2L17t9lPtAcHB6OoqAhZWVlm9XkOaw+OG1WL40bFcdywTa0NPpydnRETE4OdO3ea1hmNRuzcuRNdunSpwZ7VTZGRkQgODjY7nzk5OTh48CDP5/8TQmD8+PFYv349du3ahcjISLPymJgYODk5mZ3D06dP4/fff+c5rCU4blQtjhvqOG5UUk3f8Srz6aefCr1eL1auXClOnjwpxo4dK3x8fER6enpNd61WunXrljhy5Ig4cuSIACAWLlwojhw5Is6fPy+EEOJvf/ub8PHxERs3bhQ///yzGDx4sIiMjBS3b9+u4Z7XDn/961+Ft7e32LNnj7hy5Yppyc/PN9UZN26caNKkidi1a5f46aefRJcuXUSXLl1qsNd0N44btuG4YR+OG5VTq4MPIYRYsmSJaNKkiXB2dhadOnUSBw4cqOku1Vq7d+8WACyWxMREIcSdaXOvvfaaCAoKEnq9XvTp00ecPn26Zjtdi1g7dwDEihUrTHVu374tnnnmGeHr6yvc3NzEI488Iq5cuVJznSarOG5UHMcN+3DcqBxFCCG0u85CREREDV2tveeDiIiI6icGH0RERKQpBh9ERESkKQYfREREpCkGH0RERKQpBh9ERESkKQYfREREpCkGH0RERKQpBh9ERESkKQYfREREpCkGH0RERKQpBh9ERESkqf8DI0dXyOj74VcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize anomalies\n",
    "index = np.random.randint(1,10000)\n",
    "index = 6\n",
    "gt_image, reconstructed_image, loss = gt_reconstructed_images[index][0].squeeze(), gt_reconstructed_images[index][1].squeeze(), gt_reconstructed_images[index][2].item()\n",
    "\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(gt_image.cpu().numpy())\n",
    "ax[1].imshow(reconstructed_image.cpu().numpy())\n",
    "ax[0].set_title(\"ground truth\")\n",
    "ax[1].set_title(\"reconstructed image\")\n",
    "# plt.suptitle(f\"reconstruction loss = {np.round(loss.item(),4)}\")\n",
    "plt.suptitle(f\"reconstruction loss = {round(loss, 4)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using reconstruction error\n",
    "threshold = 0.001  # Define a threshold for anomaly detection\n",
    "model.eval()\n",
    "anomalies = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        \n",
    "        # If reconstruction error exceeds the threshold, mark it as an anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly image shape: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize anomalies\n",
    "if anomalies:\n",
    "    # Select the first anomaly and remove the channel dimension for visualization\n",
    "    anomaly_image = anomalies[0][0].squeeze()  # Remove the channel dimension (1)\n",
    "    print(f\"Anomaly image shape: {anomaly_image.shape}\")  # Optional: Check the shape of the image\n",
    "    plt.imshow(anomaly_image.cpu().numpy(), cmap='gray')  # Convert tensor to NumPy array for visualization\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No anomalies detected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
