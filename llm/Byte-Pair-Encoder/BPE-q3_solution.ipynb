{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Write a Byte Pain Encoder in Python\n",
    "\n",
    "### Problem Statement\n",
    "Implement a **Transformer model** in PyTorch by completing the required sections. The model should consist of an embedding layer, a Transformer encoder, and an output layer for sequence processing and prediction.\n",
    "\n",
    "### Requirements\n",
    "1. **Define the Transformer Model Architecture**:\n",
    "   - **Embedding Layer**:\n",
    "     - Implement a layer to transform input data into a higher-dimensional space.\n",
    "     - Use a `torch.nn.Linear` or `torch.nn.Embedding` layer to create embeddings from the input.\n",
    "   - **Transformer Encoder**:\n",
    "     - Use `torch.nn.TransformerEncoder` or `torch.nn.Transformer` to process sequences with attention.\n",
    "     - Configure parameters such as the number of attention heads and encoder layers.\n",
    "   - **Output Layer**:\n",
    "     - Add a fully connected (linear) layer to reduce the transformer's sequence output into the desired output dimension.\n",
    "\n",
    "2. **Implement the Forward Method**:\n",
    "   - Map the input to the higher-dimensional space using the embedding layer.\n",
    "   - Pass the transformed input through the Transformer encoder.\n",
    "   - Use the output layer to convert the encoded sequence into predictions.\n",
    "\n",
    "### Constraints\n",
    "- Handle input padding correctly for variable-length sequences.\n",
    "- Ensure compatibility with batch processing by correctly shaping input and output tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as', 'sh', 'hu', 'ut', 'to', 'os', 'sh']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tuple(list('asdfg'))[1:-1])\n",
    "get_all_n_grams('ashutosh',n_min=2,n_max=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o', 'w', '</w>'): 1,\n",
       " ('l', 'o', 'w', 'e', 'r', '</w>'): 1,\n",
       " ('l', 'o', 'w', 'e', 's', 't', '</w>'): 1,\n",
       " ('n', 'e', 'w', 'e', 'r', '</w>'): 1,\n",
       " ('w', 'i', 'd', 'e', 'r', '</w>'): 1,\n",
       " 'lo': 3}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('l', 'o'), 3)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pair.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low', 'lower', 'lowest', 'newer', 'wider']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('l', 'o'), ('o', 'w'), ('w', '</w>')]\n",
      "[('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')]\n",
      "[('n', 'e'), ('e', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('w', 'i'), ('i', 'd'), ('d', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "most_freq_pair_chars = lo | most_freq_pair = (('l', 'o'), 3)\n",
      "[('l', 'o'), ('o', 'w'), ('w', '</w>')]\n",
      "[('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')]\n",
      "[('n', 'e'), ('e', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('w', 'i'), ('i', 'd'), ('d', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "stats = {('l', 'o'): 3, ('o', 'w'): 3, ('w', '</w>'): 1, ('w', 'e'): 3, ('e', 'r'): 3, ('r', '</w>'): 3, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1, ('n', 'e'): 1, ('e', 'w'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1}\n",
      "most_freq_pair_chars = lo | most_freq_pair = (('l', 'o'), 3)\n",
      "vocab = {('lo', 'w', '</w>'): 1, ('lo', 'w', 'e', 'r', '</w>'): 1, ('lo', 'w', 'e', 's', 't', '</w>'): 1, ('n', 'e', 'w', 'e', 'r', '</w>'): 1, ('w', 'i', 'd', 'e', 'r', '</w>'): 1}\n",
      "====================================================================================================\n",
      "[('lo', 'w'), ('w', '</w>')]\n",
      "[('lo', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('lo', 'w'), ('w', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')]\n",
      "[('n', 'e'), ('e', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('w', 'i'), ('i', 'd'), ('d', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "stats = {('lo', 'w'): 3, ('w', '</w>'): 1, ('w', 'e'): 3, ('e', 'r'): 3, ('r', '</w>'): 3, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1, ('n', 'e'): 1, ('e', 'w'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1}\n",
      "most_freq_pair_chars = low | most_freq_pair = (('lo', 'w'), 3)\n",
      "vocab = {('low', '</w>'): 1, ('low', 'e', 'r', '</w>'): 1, ('low', 'e', 's', 't', '</w>'): 1, ('n', 'e', 'e', 'r', '</w>'): 1, ('i', 'd', 'e', 'r', '</w>'): 1}\n",
      "====================================================================================================\n",
      "[('low', '</w>')]\n",
      "[('low', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('low', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')]\n",
      "[('n', 'e'), ('e', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('i', 'd'), ('d', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "stats = {('low', '</w>'): 1, ('low', 'e'): 2, ('e', 'r'): 3, ('r', '</w>'): 3, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1, ('n', 'e'): 1, ('e', 'e'): 1, ('i', 'd'): 1, ('d', 'e'): 1}\n",
      "most_freq_pair_chars = er | most_freq_pair = (('e', 'r'), 3)\n",
      "vocab = {('low', '</w>'): 1, ('low', 'er', '</w>'): 1, ('low', 'e', 's', 't', '</w>'): 1, ('n', 'e', 'er', '</w>'): 1, ('i', 'd', 'er', '</w>'): 1}\n",
      "====================================================================================================\n",
      "[('low', '</w>')]\n",
      "[('low', 'er'), ('er', '</w>')]\n",
      "[('low', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')]\n",
      "[('n', 'e'), ('e', 'er'), ('er', '</w>')]\n",
      "[('i', 'd'), ('d', 'er'), ('er', '</w>')]\n",
      "stats = {('low', '</w>'): 1, ('low', 'er'): 1, ('er', '</w>'): 3, ('low', 'e'): 1, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1, ('n', 'e'): 1, ('e', 'er'): 1, ('i', 'd'): 1, ('d', 'er'): 1}\n",
      "most_freq_pair_chars = er</w> | most_freq_pair = (('er', '</w>'), 3)\n",
      "vocab = {('low',): 1, ('low', 'er</w>'): 1, ('low', 'e', 's', 't'): 1, ('n', 'e', 'er</w>'): 1, ('i', 'd', 'er</w>'): 1}\n",
      "====================================================================================================\n",
      "[]\n",
      "[('low', 'er</w>')]\n",
      "[('low', 'e'), ('e', 's'), ('s', 't')]\n",
      "[('n', 'e'), ('e', 'er</w>')]\n",
      "[('i', 'd'), ('d', 'er</w>')]\n",
      "stats = {('low', 'er</w>'): 1, ('low', 'e'): 1, ('e', 's'): 1, ('s', 't'): 1, ('n', 'e'): 1, ('e', 'er</w>'): 1, ('i', 'd'): 1, ('d', 'er</w>'): 1}\n",
      "most_freq_pair_chars = lower</w> | most_freq_pair = (('low', 'er</w>'), 1)\n",
      "vocab = {('low',): 1, ('lower</w>',): 1, ('low', 'e', 's', 't'): 1, ('n', 'e'): 1, ('i', 'd'): 1}\n",
      "====================================================================================================\n",
      "[]\n",
      "[]\n",
      "[('low', 'e'), ('e', 's'), ('s', 't')]\n",
      "[('n', 'e')]\n",
      "[('i', 'd')]\n",
      "stats = {('low', 'e'): 1, ('e', 's'): 1, ('s', 't'): 1, ('n', 'e'): 1, ('i', 'd'): 1}\n",
      "most_freq_pair_chars = lowe | most_freq_pair = (('low', 'e'), 1)\n",
      "vocab = {('low',): 1, ('lower</w>',): 1, ('lowe', 's', 't'): 1, ('n',): 1, ('i', 'd'): 1}\n",
      "====================================================================================================\n",
      "[]\n",
      "[]\n",
      "[('lowe', 's'), ('s', 't')]\n",
      "[]\n",
      "[('i', 'd')]\n",
      "stats = {('lowe', 's'): 1, ('s', 't'): 1, ('i', 'd'): 1}\n",
      "most_freq_pair_chars = lowes | most_freq_pair = (('lowe', 's'), 1)\n",
      "vocab = {('low',): 1, ('lower</w>',): 1, ('lowes', 't'): 1, ('n',): 1, ('i', 'd'): 1}\n",
      "====================================================================================================\n",
      "[]\n",
      "[]\n",
      "[('lowes', 't')]\n",
      "[]\n",
      "[('i', 'd')]\n",
      "stats = {('lowes', 't'): 1, ('i', 'd'): 1}\n",
      "most_freq_pair_chars = lowest | most_freq_pair = (('lowes', 't'), 1)\n",
      "vocab = {('low',): 1, ('lower</w>',): 1, ('lowest',): 1, ('n',): 1, ('i', 'd'): 1}\n",
      "====================================================================================================\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('i', 'd')]\n",
      "stats = {('i', 'd'): 1}\n",
      "most_freq_pair_chars = id | most_freq_pair = (('i', 'd'), 1)\n",
      "vocab = {('low',): 1, ('lower</w>',): 1, ('lowest',): 1, ('n',): 1, ('id',): 1}\n",
      "====================================================================================================\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "stats = {}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pair \u001b[38;5;241m=\u001b[39m get_stats(vocab)\n\u001b[1;32m      3\u001b[0m merge_vocab(pair, vocab)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mbyte_pair_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[172], line 97\u001b[0m, in \u001b[0;36mbyte_pair_encoding\u001b[0;34m(corpus, num_merges)\u001b[0m\n\u001b[1;32m     95\u001b[0m stats \u001b[38;5;241m=\u001b[39m get_stats(vocab)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[172], line 80\u001b[0m, in \u001b[0;36mmerge_vocab\u001b[0;34m(pair, vocab)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Merges the most frequent pair into a single symbol.\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m sorted_pair_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(pair\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 80\u001b[0m most_freq_pair \u001b[38;5;241m=\u001b[39m \u001b[43msorted_pair_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     81\u001b[0m most_freq_pair_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(most_freq_pair[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_freq_pair_chars = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmost_freq_pair_chars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | most_freq_pair = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmost_freq_pair\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab(corpus)\n",
    "pair = get_stats(vocab)\n",
    "merge_vocab(pair, vocab)\n",
    "byte_pair_encoding(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('l', 'o'), ('o', 'w'), ('w', '</w>')]\n",
      "[('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('l', 'o'), ('o', 'w'), ('w', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')]\n",
      "[('n', 'e'), ('e', 'w'), ('w', 'e'), ('e', 'r'), ('r', '</w>')]\n",
      "[('w', 'i'), ('i', 'd'), ('d', 'e'), ('e', 'r'), ('r', '</w>')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('l', 'o'), 3),\n",
       " (('o', 'w'), 3),\n",
       " (('w', 'e'), 3),\n",
       " (('e', 'r'), 3),\n",
       " (('r', '</w>'), 3),\n",
       " (('w', '</w>'), 1),\n",
       " (('e', 's'), 1),\n",
       " (('s', 't'), 1),\n",
       " (('t', '</w>'), 1),\n",
       " (('n', 'e'), 1),\n",
       " (('e', 'w'), 1),\n",
       " (('w', 'i'), 1),\n",
       " (('i', 'd'), 1),\n",
       " (('d', 'e'), 1)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_vocab(corpus, cased=False):\n",
    "    \"\"\"Creates a vocabulary with words split into characters and a special end-of-word token.\"\"\"\n",
    "    eow_char = '</w>'\n",
    "    from collections import Counter\n",
    "    if cased == True:\n",
    "        corpus = [lower(word.strip()) for word in corpus]\n",
    "    word_frequency = Counter(corpus)\n",
    "    vocab = {}\n",
    "    for word in word_frequency:\n",
    "        chars = list(word)\n",
    "        chars.append(eow_char)\n",
    "        chars = tuple(chars)\n",
    "        vocab[chars] = word_frequency.get(word, 1)\n",
    "    return vocab\n",
    "\n",
    "# def get_all_n_grams(word):\n",
    "#     n_grams = []\n",
    "#     for n_gram in range(len(word)):\n",
    "#         for i in range(len(word)-n_gram):\n",
    "#             n_grams.append(word[i:i+n_gram+1])\n",
    "#     return n_grams\n",
    "\n",
    "\n",
    "def get_all_n_grams(word, n_min=None, n_max=None):\n",
    "    n_grams = []\n",
    "    if n_max == None:\n",
    "        n_max = len(word)\n",
    "    if n_min == None:\n",
    "        n_min = 1\n",
    "    for n_gram in range(n_min, n_max+1):\n",
    "        for i in range(len(word)-(n_gram-1)):\n",
    "            n_grams.append(word[i:i+n_gram])\n",
    "    print(n_grams)\n",
    "    return n_grams\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Counts frequency of adjacent symbol pairs.\"\"\"\n",
    "    stats = {}\n",
    "    for characters, word_count in vocab.items():\n",
    "        # word = ''.join(characters[:-1])\n",
    "        # word_ngrams = get_all_n_grams(word)\n",
    "        word_ngrams = get_all_n_grams(characters, n_min=2, n_max=2)\n",
    "        for pair in word_ngrams:\n",
    "            stats[pair] = stats.get(pair, 0) + word_count\n",
    "    return stats\n",
    "\n",
    "def get_merged_key(key, most_freq_pair):\n",
    "    \"\"\"\n",
    "    1. abcde, bc\n",
    "    2. abcde, fg\n",
    "    3. abbcdd, bb\n",
    "    \"\"\"\n",
    "    first_elem = most_freq_pair[0]\n",
    "    second_elem = most_freq_pair[1]\n",
    "    first_elem_already_match = False\n",
    "    new_key = []\n",
    "    for char in key:\n",
    "        if char == second_elem:\n",
    "            if first_elem_already_match:\n",
    "                # new_key = new_key[:-1] + [first_elem+second_elem]\n",
    "                new_key[-1] = first_elem+second_elem\n",
    "        elif char == first_elem:\n",
    "            first_elem_already_match = True\n",
    "            new_key.append(char)\n",
    "        else:\n",
    "            new_key.append(char)\n",
    "\n",
    "        if first_elem_already_match == True:\n",
    "            first_elem_already_match = False\n",
    "        if char == first_elem:\n",
    "            first_elem_already_match = True\n",
    "    return tuple(new_key)\n",
    "\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"Merges the most frequent pair into a single symbol.\"\"\"\n",
    "    sorted_pair_copy = sorted(pair.items(), key=lambda x: x[1], reverse=True)\n",
    "    most_freq_pair = sorted_pair_copy[0]\n",
    "    most_freq_pair_chars = ''.join(most_freq_pair[0])\n",
    "    print(f\"most_freq_pair_chars = {most_freq_pair_chars} | most_freq_pair = {most_freq_pair}\")\n",
    "    new_vocab = {}\n",
    "    for key in vocab:\n",
    "        val = vocab.get(key)\n",
    "        new_key = get_merged_key(key, most_freq_pair[0])\n",
    "        new_vocab[new_key] = val\n",
    "    # new_vocab.update(vocab)\n",
    "    return new_vocab\n",
    "\n",
    "def byte_pair_encoding(corpus, num_merges=10):\n",
    "    \"\"\"Performs BPE on a corpus.\"\"\"\n",
    "    vocab = get_vocab(corpus)\n",
    "    for i in range(num_merges):\n",
    "        stats = get_stats(vocab)\n",
    "        print(f\"stats = {stats}\")\n",
    "        vocab = merge_vocab(stats, vocab)\n",
    "        print(f\"vocab = {vocab}\")\n",
    "        print(\"=\"*100)\n",
    "    return vocab\n",
    "\n",
    "# Example usage\n",
    "corpus = [\"low\", \"lower\", \"lowest\", \"newer\", \"wider\"]\n",
    "# get_vocab(corpus)\n",
    "sorted(get_stats(get_vocab(corpus)).items(), key=lambda x: x[1], reverse=True)\n",
    "# get_stats(get_vocab(corpus))\n",
    "# final_vocab, merge_operations = byte_pair_encoding(corpus, num_merges=10)\n",
    "\n",
    "# print(\"\\nFinal Vocabulary:\")\n",
    "# for word in final_vocab:\n",
    "#     print(' '.join(word), \":\", final_vocab[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('t', 'e', 's', 't', '</w>'): 1}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{('t', 'e', 's', 't', '</w>'): 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('t', 'e', 's', 't', '</w>'): 1}\n",
      "✓ test_get_vocab passed\n",
      "[('t', 'e'), ('e', 's'), ('s', 't'), ('t', '</w>')]\n",
      "✓ test_get_stats passed\n",
      "most_freq_pair_chars = es | most_freq_pair = (('e', 's'), 2)\n",
      "{('t', 'es', 't', '</w>'): 1}\n",
      "✓ test_merge_vocab passed\n"
     ]
    }
   ],
   "source": [
    "def test_get_vocab():\n",
    "    corpus = [\"test\"]\n",
    "    vocab = get_vocab(corpus)\n",
    "    print(vocab)\n",
    "    assert vocab == {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    print(\"✓ test_get_vocab passed\")\n",
    "\n",
    "def test_get_stats():\n",
    "    vocab = {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    stats = get_stats(vocab)\n",
    "    expected = {\n",
    "        ('t', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', 't'): 1,\n",
    "        ('t', '</w>'): 1\n",
    "    }\n",
    "    assert stats == expected\n",
    "    print(\"✓ test_get_stats passed\")\n",
    "\n",
    "def test_merge_vocab():\n",
    "    vocab = {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    stats = {\n",
    "        ('t', 'e'): 1,\n",
    "        ('e', 's'): 2,\n",
    "        ('s', 't'): 1,\n",
    "        ('t', '</w>'): 1\n",
    "    }\n",
    "    merged = merge_vocab(stats, vocab)\n",
    "    # merged = merge_vocab(('e', 's'), vocab)\n",
    "    print(merged)\n",
    "    expected = {('t', 'es', 't', '</w>'): 1}\n",
    "    assert merged == expected\n",
    "    print(\"✓ test_merge_vocab passed\")\n",
    "\n",
    "def test_bpe_sequence():\n",
    "    corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "    final_vocab, merges = byte_pair_encoding(corpus, num_merges=5)\n",
    "    assert isinstance(final_vocab, dict)\n",
    "    assert all(isinstance(pair, tuple) for pair in merges)\n",
    "    assert len(merges) == 5\n",
    "    print(\"✓ test_bpe_sequence passed\")\n",
    "\n",
    "# Run all tests\n",
    "test_get_vocab()\n",
    "test_get_stats()\n",
    "test_merge_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
