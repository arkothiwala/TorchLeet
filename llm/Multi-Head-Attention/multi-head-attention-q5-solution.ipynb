{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Attention from Scratch\n",
    "### Problem Statement\n",
    "Multi-Head Attention (MHA) is the bread-and-butter of the Transformer architecture. It enables the model to **jointly attend** to information from different representation subspaces at different positions.\n",
    "\n",
    "Your goal is to implement MHA from scratch using PyTorch, simulating exactly what `torch.nn.MultiheadAttention` does ‚Äî projecting Q, K, V for each head, computing attention weights, applying them to V, and concatenating the outputs across all heads.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. **Linear Projections for Q, K, V**\n",
    "   - Project input `q`, `k`, `v` into a total of `d_model` dimensions.\n",
    "   - Split them into `num_heads` of `d_head = d_model // num_heads` each.\n",
    "\n",
    "2. **Scaled Dot-Product Attention per Head**\n",
    "   - Compute attention scores:  \n",
    "     `scores = Q @ K·µÄ / sqrt(d_head)`\n",
    "   - Apply an optional `mask` before softmax.\n",
    "   - Use the scores to weight `V`.\n",
    "\n",
    "3. **Combine the Heads**\n",
    "   - Concatenate the outputs of all heads.\n",
    "   - Apply a final linear projection to restore the shape: `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "4. **Validate Against PyTorch‚Äôs Reference**\n",
    "   - Test your output against `torch.nn.MultiheadAttention` using the same input tensors.\n",
    "   - Check for numerical closeness using `torch.allclose()`.\n",
    "\n",
    "---\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- ‚úÖ Use only PyTorch operations.\n",
    "- ‚úÖ Make sure all tensors are reshaped properly when splitting and combining heads.\n",
    "- ‚úÖ Support optional masking.\n",
    "- ‚úÖ Must match `torch.nn.MultiheadAttention` output when heads and shape are aligned.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint</summary>\n",
    "\n",
    "  - Use `.view()` and `.transpose()` to shape Q, K, V to `(batch_size, num_heads, seq_len, d_head)`.\n",
    "  - Softmax should be applied over the **last dimension** (attention scores across sequence).\n",
    "  - Use `.contiguous().view()` to flatten the multi-head outputs back into `(batch_size, seq_len, d_model)`.\n",
    "  - Match PyTorch‚Äôs behavior using the same projections and batch-first format.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Synthetic data\n",
    "torch.manual_seed(42)\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "print(q.shape)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def multi_head_attention(q, k, v, num_heads, d_model, mask=None):\n",
    "#     \"\"\"\n",
    "#     Implements multi-head attention.\n",
    "    \n",
    "#     Args:\n",
    "#         q (Tensor): Query tensor of shape (batch_size, seq_len, d_model)\n",
    "#         k (Tensor): Key tensor of shape (batch_size, seq_len, d_model)\n",
    "#         v (Tensor): Value tensor of shape (batch_size, seq_len, d_model)\n",
    "#         num_heads (int): Number of attention heads\n",
    "#         d_model (int): Total embedding dimension\n",
    "#         mask (Tensor, optional): Masking tensor for attention\n",
    "        \n",
    "#     Returns:\n",
    "#         Tensor: Multi-head attention output of shape (batch_size, seq_len, d_model)\n",
    "#     \"\"\"\n",
    "#     batch_size, seq_len, d_q = q.shape\n",
    "\n",
    "#     # original paper doesn't mention presence of bias term in the projection layer\n",
    "#     Wq = torch.nn.Linear(in_features=q.shape[-1], out_features=d_model, bias=False)\n",
    "#     Wk = torch.nn.Linear(in_features=k.shape[-1], out_features=d_model, bias=False)\n",
    "#     Wv = torch.nn.Linear(in_features=v.shape[-1], out_features=d_model, bias=False)\n",
    "#     Wc = torch.nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "#     # # projections for Q, K, V metrics\n",
    "#     # q = q @ Wq # (B, L_k, d_model)\n",
    "#     # k = k @ Wk # (B, L_k, d_model)\n",
    "#     # v = q @ Wv # (B, L_v, d_model)\n",
    "\n",
    "#     # projections for Q, K, V metrics\n",
    "#     q = Wq(q) # (B, L_k, d_model)\n",
    "#     k = Wk(k) # (B, L_k, d_model)\n",
    "#     v = Wv(v) # (B, L_v, d_model)\n",
    "\n",
    "#     d_mha = d_model // num_heads\n",
    "#     print(f\"d_model={d_model} | num_heads={num_heads} | d_mha={d_mha}\")\n",
    "\n",
    "#     assert d_mha*num_heads == d_model, \"incompatible num_head and d_model conbination\"\n",
    "\n",
    "#     # split Q, K, V into multiple heads\n",
    "#     q = q.view(batch_size, seq_len, num_heads, d_mha) # (B, L_k, H, d_mha)\n",
    "#     k = k.view(batch_size, seq_len, num_heads, d_mha) # (B, L_k, H, d_mha)\n",
    "#     v = v.view(batch_size, seq_len, num_heads, d_mha) # (B, L_k, H, d_mha)\n",
    "\n",
    "#     # reshape to move number of heads to 2nd axis\n",
    "#     q = q.transpose(1,2) # (B, H, L_k, d_mha)\n",
    "#     k = k.transpose(1,2) # (B, H, L_k, d_mha)\n",
    "#     v = v.transpose(1,2) # (B, H, L_k, d_mha)\n",
    "\n",
    "#     # apply attention\n",
    "#     attention_per_head = F.scaled_dot_product_attention(q, k, v, attn_mask=mask) # (B, H, L_k, d_mha)\n",
    "#     attention_per_head = attention_per_head.permute(0,2,1,3) # (B, L_k, H, d_mha)\n",
    "#     print(f\"attention_per_head.shape = {attention_per_head.shape}\")\n",
    "#     print(f\"(batch_size, seq_len, d_model) = {(batch_size, seq_len, d_model)}\")\n",
    "#     attention_concatenated = attention_per_head.reshape(batch_size, seq_len, d_model) # (B, L_k, d_model)\n",
    "    \n",
    "#     # mha = attention_concatenated @ Wc # (B, L_k, d_model)\n",
    "#     mha = Wc(attention_concatenated) # (B, L_k, d_model)\n",
    "#     return mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, bias):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.bias = bias\n",
    "        self.d_mha = self.d_model // self.num_heads\n",
    "        assert self.d_mha*self.num_heads == self.d_model, \"incompatible num_head and d_model conbination\"\n",
    "        print(f\"d_model={self.d_model} | num_heads={self.num_heads} | d_mha={self.d_mha}\")\n",
    "\n",
    "        self.Wq = torch.nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n",
    "        self.Wk = torch.nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n",
    "        self.Wv = torch.nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n",
    "        self.Wc = torch.nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, d_q = q.shape\n",
    "\n",
    "        # projections for Q, K, V metrics\n",
    "        q = self.Wq(q) # (B, L_k, d_model)\n",
    "        k = self.Wk(k) # (B, L_k, d_model)\n",
    "        v = self.Wv(v) # (B, L_v, d_model)\n",
    "\n",
    "\n",
    "        # split Q, K, V into multiple heads\n",
    "        q = q.view(batch_size, q.shape[-2], self.num_heads, self.d_mha) # (B, L_k, H, d_mha)\n",
    "        k = k.view(batch_size, k.shape[-2], self.num_heads, self.d_mha) # (B, L_k, H, d_mha)\n",
    "        v = v.view(batch_size, v.shape[-2], self.num_heads, self.d_mha) # (B, L_k, H, d_mha)\n",
    "\n",
    "        # reshape to move number of heads to 2nd axis\n",
    "        q = q.transpose(1,2) # (B, H, L_k, d_mha)\n",
    "        k = k.transpose(1,2) # (B, H, L_k, d_mha)\n",
    "        v = v.transpose(1,2) # (B, H, L_k, d_mha)\n",
    "\n",
    "        # apply attention\n",
    "        attention_per_head = F.scaled_dot_product_attention(q, k, v, attn_mask=mask) # (B, H, L_k, d_mha)\n",
    "        attention_per_head = attention_per_head.permute(0,2,1,3) # (B, L_k, H, d_mha)\n",
    "        print(f\"attention_per_head.shape = {attention_per_head.shape}\")\n",
    "        print(f\"(batch_size, seq_len, d_model) = {(batch_size, seq_len, self.d_model)}\")\n",
    "        attention_concatenated = attention_per_head.reshape(batch_size, seq_len, self.d_model) # (B, L_k, d_model)\n",
    "        \n",
    "        # mha = attention_concatenated @ Wc # (B, L_k, d_model)\n",
    "        mha = self.Wc(attention_concatenated) # (B, L_k, d_model)\n",
    "        return mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_head_attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testing on data & compare\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_custom \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_head_attention\u001b[49m(q, k, v, num_heads, d_model)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(output_custom)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2025\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_head_attention' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing on data & compare\n",
    "output_custom = multi_head_attention(q, k, v, num_heads, d_model)\n",
    "# print(output_custom)\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "multihead_attn = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, bias=False, batch_first=True)\n",
    "output, _ = multihead_attn(q, k, v)\n",
    "print(output)\n",
    "\n",
    "assert torch.allclose(output_custom, output, atol=1e-08, rtol=1e-05) # Check if they are close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model=8 | num_heads=2 | d_mha=4\n",
      "attention_per_head.shape = torch.Size([3, 4, 2, 4])\n",
      "(batch_size, seq_len, d_model) = (3, 4, 8)\n",
      "Maximum difference: 0.0\n",
      "‚úÖ SUCCESS: Your implementation matches PyTorch exactly!\n"
     ]
    }
   ],
   "source": [
    "# # # --- 2. The Setup ---\n",
    "# # d_model = 512\n",
    "# # num_heads = 8\n",
    "# # seq_len = 10\n",
    "# # batch_size = 2\n",
    "\n",
    "# # Synthetic data\n",
    "# torch.manual_seed(42)\n",
    "# batch_size = 3\n",
    "# seq_len = 4\n",
    "# d_model = 8\n",
    "# num_heads = 2\n",
    "\n",
    "# # Input Data (Random)\n",
    "# x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# --- 3. Instantiate Both Models ---\n",
    "# PyTorch Native Implementation\n",
    "# We use batch_first=True to match your shape convention (Batch, Seq, Feature)\n",
    "pytorch_mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True, bias=True)\n",
    "\n",
    "# Your Custom Implementation\n",
    "custom_mha = CustomMultiHeadAttention(d_model=d_model, num_heads=num_heads, bias=True)\n",
    "\n",
    "# --- 4. THE MAGIC STEP: Copy Weights ---\n",
    "# PyTorch stores Q, K, V weights in one giant matrix called `in_proj_weight`\n",
    "# shape: [3 * d_model, d_model]. We chunk it into 3 parts.\n",
    "with torch.no_grad():\n",
    "    # 1. Slice the weights\n",
    "    W_q_check, W_k_check, W_v_check = pytorch_mha.in_proj_weight.chunk(3, dim=0)\n",
    "    b_q_check, b_k_check, b_v_check = pytorch_mha.in_proj_bias.chunk(3, dim=0)\n",
    "\n",
    "    # 2. Copy to your custom layers\n",
    "    custom_mha.Wq.weight.copy_(W_q_check)\n",
    "    custom_mha.Wq.bias.copy_(b_q_check)\n",
    "    \n",
    "    custom_mha.Wk.weight.copy_(W_k_check)\n",
    "    custom_mha.Wk.bias.copy_(b_k_check)\n",
    "    \n",
    "    custom_mha.Wv.weight.copy_(W_v_check)\n",
    "    custom_mha.Wv.bias.copy_(b_v_check)\n",
    "    \n",
    "    # 3. Copy the Output Projection weights (PyTorch calls it `out_proj`)\n",
    "    custom_mha.Wc.weight.copy_(pytorch_mha.out_proj.weight)\n",
    "    custom_mha.Wc.bias.copy_(pytorch_mha.out_proj.bias)\n",
    "\n",
    "# --- 5. Run Comparison ---\n",
    "pytorch_mha.eval() # Disable dropout\n",
    "custom_mha.eval()  # Disable dropout\n",
    "\n",
    "# PyTorch Forward\n",
    "# output is a tuple (attn_output, attn_output_weights), we only need the first one\n",
    "pytorch_out, _ = pytorch_mha(q,k,v, need_weights=False)\n",
    "\n",
    "# Custom Forward\n",
    "custom_out = custom_mha(q,k,v)\n",
    "\n",
    "# --- 6. Verify ---\n",
    "# We check if the difference is negligible (e.g., less than 1e-5)\n",
    "diff = (pytorch_out - custom_out).abs().max()\n",
    "print(f\"Maximum difference: {diff.item()}\")\n",
    "\n",
    "if torch.allclose(pytorch_out, custom_out, atol=1e-5):\n",
    "    print(\"‚úÖ SUCCESS: Your implementation matches PyTorch exactly!\")\n",
    "else:\n",
    "    print(\"‚ùå FAILURE: The outputs diverge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
