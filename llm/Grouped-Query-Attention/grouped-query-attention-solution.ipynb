{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Attention from Scratch\n",
    "### üß† Problem Statement\n",
    "Standard Multi-Head Attention (MHA) assigns a separate query, key, and value projection to each attention head. But that‚Äôs not always the most efficient approach. \n",
    "\n",
    "Enter **Grouped Query Attention (GQA)** ‚Äî a clever mechanism where you use more query heads than key-value heads. This reduces compute/memory costs while still allowing for fine-grained query specialization.\n",
    "\n",
    "Your task is to **implement GQA from scratch** and validate it against PyTorch‚Äôs `MultiheadAttention` under the special case where GQA behaves identically to MHA (i.e., when `num_query_heads == num_query_groups`).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Requirements\n",
    "\n",
    "1. **Define the GQA Mechanism**\n",
    "   - Create a function `grouped_query_attention(q, k, v, num_query_groups, d_model, mask=None)`.\n",
    "   - Project `q`, `k`, and `v` using linear layers:\n",
    "     - Q projection ‚Üí all query heads.\n",
    "     - K/V projection ‚Üí shared across grouped key/value heads.\n",
    "   - Use `repeat_interleave()` to expand grouped K/V heads to match the number of Q heads.\n",
    "\n",
    "2. **Compute Attention**\n",
    "   - Apply scaled dot-product attention using `Q @ K·µÄ / sqrt(d_head)`.\n",
    "   - Support optional masking.\n",
    "   - Return output by concatenating heads and applying the output projection.\n",
    "\n",
    "3. **Validate Against MHA**\n",
    "   - Test your implementation using synthetic tensors.\n",
    "   - Compare your output to `torch.nn.MultiheadAttention` where GQA degenerates to MHA (`num_query_heads == num_query_groups`).\n",
    "   - Assert that both outputs match numerically.\n",
    "\n",
    "---\n",
    "\n",
    "### üìè Constraints\n",
    "\n",
    "- ‚úÖ Use only PyTorch (no external libraries like xformers or HuggingFace).\n",
    "- ‚úÖ Output shape must be `(batch_size, seq_len, d_model)`.\n",
    "- ‚úÖ Support optional attention masking.\n",
    "- ‚úÖ Validate output against `torch.nn.MultiheadAttention` for correctness.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint</summary>\n",
    "\n",
    "  - Use `nn.Linear(d_model, d_model)` for projecting `q`, `k`, and `v`.\n",
    "  - When `num_query_heads > num_query_groups`, use `.repeat_interleave()` to duplicate each group‚Äôs `K`/`V` to match query head count.\n",
    "  - Final output: reshape the multi-head outputs to `(batch_size, seq_len, d_model)` and apply the output projection layer.\n",
    "  - Test with `num_query_heads == num_query_groups` to confirm it behaves like MHA.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Synthetic data\n",
    "torch.manual_seed(42)\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "print(q.shape)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def grouped_query_attention(q, k, v, num_query_groups, d_model, mask=None):\n",
    "    \"\"\"\n",
    "    Implements Grouped Query Attention (GQA).\n",
    "\n",
    "    Args:\n",
    "        q (Tensor): Query tensor of shape (batch_size, seq_len, d_model)\n",
    "        k (Tensor): Key tensor of shape (batch_size, seq_len, d_model)\n",
    "        v (Tensor): Value tensor of shape (batch_size, seq_len, d_model)\n",
    "        num_query_groups (int): Number of key/value groups (fewer than query groups)\n",
    "        d_model (int): Total embedding dimension\n",
    "        mask (Tensor, optional): Masking tensor for attention\n",
    "\n",
    "    Returns:\n",
    "        Tensor: GQA output of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomGroupedQueryAttention(torch.nn.Module):\n",
    "    def __init__(self, num_query_heads, num_query_groups, d_model, bias):\n",
    "        super().__init__()\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.num_query_groups = num_query_groups\n",
    "        self.d_model = d_model\n",
    "        self.bias = bias\n",
    "        self.dq_mha = self.d_model // self.num_query_heads\n",
    "        self.dk_groups = self.dq_mha*self.num_query_groups\n",
    "        self.q_kv_groups_ratio = self.num_query_heads//self.num_query_groups\n",
    "        \n",
    "        assert self.dq_mha*self.num_query_heads == self.d_model, \"incompatible num_head and d_model conbination\"\n",
    "        print(f\"d_model={self.d_model} | num_heads={self.num_query_heads} | d_mha={self.dq_mha}\")\n",
    "\n",
    "        self.Wq = torch.nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n",
    "        self.Wk = torch.nn.Linear(in_features=self.d_model, out_features=self.dk_groups, bias=self.bias)\n",
    "        self.Wv = torch.nn.Linear(in_features=self.d_model, out_features=self.dk_groups, bias=self.bias)\n",
    "        self.Wc = torch.nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len_q, d_q = q.shape\n",
    "        batch_size, seq_len_k, d_k = k.shape\n",
    "        batch_size, seq_len_v, d_v = v.shape\n",
    "\n",
    "        # projections for Q, K, V metrics\n",
    "        q = self.Wq(q) # (B, L_q, d_model)\n",
    "        k = self.Wk(k) # (B, L_k, dk_groups)\n",
    "        v = self.Wv(v) # (B, L_v, dk_groups)\n",
    "\n",
    "\n",
    "        # split Q, K, V into multiple heads\n",
    "        q = q.view(batch_size, q.shape[-2], self.num_query_heads, self.dq_mha) # (B, L_q, H_q, dq_mha)\n",
    "        k = k.view(batch_size, k.shape[-2], self.num_query_groups, self.dq_mha) # (B, L_k, H_g, dq_mha)\n",
    "        v = v.view(batch_size, v.shape[-2], self.num_query_groups, self.dq_mha) # (B, L_v, H_g, dq_mha)\n",
    "\n",
    "        # reshape to move number of heads to 2nd axis\n",
    "        q = q.transpose(1,2) # (B, H_q, L_q, dq_mha)\n",
    "        k = k.transpose(1,2) # (B, H_g, L_k, dq_mha)\n",
    "        v = v.transpose(1,2) # (B, H_g, L_v, dq_mha)\n",
    "\n",
    "        k = k.repeat_interleave(repeats=self.q_kv_groups_ratio, dim=-3)\n",
    "        v = v.repeat_interleave(repeats=self.q_kv_groups_ratio, dim=-3)\n",
    "\n",
    "        print(f\"q.shape = {q.shape} | k.shape = {k.shape} | v.shape = {v.shape}\")\n",
    "\n",
    "        # apply attention\n",
    "        attention_per_head = F.scaled_dot_product_attention(q, k, v, attn_mask=mask) # (B, H_q, L_q, dq_mha)\n",
    "        attention_per_head = attention_per_head.permute(0,2,1,3) # (B, L_q, H_q, d_mha)\n",
    "        print(f\"attention_per_head.shape = {attention_per_head.shape}\")\n",
    "        print(f\"(batch_size, seq_len, d_model) = {(batch_size, seq_len, self.d_model)}\")\n",
    "        attention_concatenated = attention_per_head.reshape(batch_size, seq_len, self.d_model) # (B, L_q, d_model)\n",
    "        \n",
    "        # mha = attention_concatenated @ Wc # (B, L_k, d_model)\n",
    "        mha = self.Wc(attention_concatenated) # (B, L_k, d_model)\n",
    "        return mha, attention_per_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_ref.shape = torch.Size([3, 4, 8])\n",
      "d_model=8 | num_heads=4 | d_mha=2\n",
      "q.shape = torch.Size([3, 4, 4, 2]) | k.shape = torch.Size([3, 4, 4, 2]) | v.shape = torch.Size([3, 4, 4, 2])\n",
      "attention_per_head.shape = torch.Size([3, 4, 4, 2])\n",
      "(batch_size, seq_len, d_model) = (3, 4, 8)\n",
      "output_custom.shape = torch.Size([3, 4, 8])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_custom.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_custom\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compare\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(output_custom, output_ref, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Temporarily set\n",
    "num_query_heads = 4\n",
    "num_query_groups = 2  # => GQA behaves like MHA\n",
    "\n",
    "# Use same d_model and input\n",
    "multihead_attn = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, bias=False, batch_first=True)\n",
    "output_ref, _ = multihead_attn(q, k, v)\n",
    "print(f\"output_ref.shape = {output_ref.shape}\")\n",
    "# output_custom = grouped_query_attention(q, k, v, num_query_heads=num_query_heads, num_query_groups=num_query_groups, d_model=d_model)\n",
    "custom_gqa = CustomGroupedQueryAttention(num_query_heads, num_query_groups, d_model, bias=False)\n",
    "output_custom, _ = custom_gqa(q, k, v)\n",
    "print(f\"output_custom.shape = {output_custom.shape}\")\n",
    "\n",
    "# Compare\n",
    "assert torch.allclose(output_custom, output_ref, atol=1e-8, rtol=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
